{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation + working convlstm G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, ConvLSTM2D, Conv3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, 3, 64, 64, 40)     59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 3, 64, 64, 1)      1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f3b4c4bb5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f3b4c4bb5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3b4c3e0050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3b4c3e0050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3cd362e8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3cd362e8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "9 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.998465] [MSE loss: 0.971310]\n",
      "19 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.998522] [MSE loss: 0.972073]\n",
      "29 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.998570] [MSE loss: 0.972697]\n",
      "39 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.998610] [MSE loss: 0.973252]\n",
      "49 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.998644] [MSE loss: 0.971871]\n",
      "59 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.998674] [MSE loss: 0.971652]\n",
      "69 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.998700] [MSE loss: 0.972282]\n",
      "79 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.998722] [MSE loss: 0.973596]\n",
      "89 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.998742] [MSE loss: 0.973649]\n",
      "99 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.998759] [MSE loss: 0.972213]\n",
      "109 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.998775] [MSE loss: 0.974197]\n",
      "119 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.998789] [MSE loss: 0.973297]\n",
      "129 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.998802] [MSE loss: 0.974037]\n",
      "139 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.998813] [MSE loss: 0.972899]\n",
      "149 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.998824] [MSE loss: 0.972744]\n",
      "159 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.998833] [MSE loss: 0.973413]\n",
      "169 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.998842] [MSE loss: 0.972419]\n",
      "179 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.998850] [MSE loss: 0.972685]\n",
      "189 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.998857] [MSE loss: 0.973913]\n",
      "199 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.998864] [MSE loss: 0.972147]\n",
      "209 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.998870] [MSE loss: 0.972615]\n",
      "219 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.998876] [MSE loss: 0.973254]\n",
      "229 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.998881] [MSE loss: 0.973748]\n",
      "239 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.998886] [MSE loss: 0.972314]\n",
      "249 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.998891] [MSE loss: 0.973301]\n",
      "259 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.998895] [MSE loss: 0.973878]\n",
      "269 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.998900] [MSE loss: 0.972637]\n",
      "279 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.998903] [MSE loss: 0.972005]\n",
      "289 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.998907] [MSE loss: 0.973404]\n",
      "299 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.998910] [MSE loss: 0.974101]\n",
      "309 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.998914] [MSE loss: 0.974573]\n",
      "319 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.998917] [MSE loss: 0.973621]\n",
      "329 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.998920] [MSE loss: 0.972950]\n",
      "339 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.998922] [MSE loss: 0.974105]\n",
      "349 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.998925] [MSE loss: 0.971985]\n",
      "359 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.998927] [MSE loss: 0.973296]\n",
      "369 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.998930] [MSE loss: 0.973060]\n",
      "379 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.998932] [MSE loss: 0.972354]\n",
      "389 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.998934] [MSE loss: 0.973813]\n",
      "399 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.998936] [MSE loss: 0.972485]\n",
      "409 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.998938] [MSE loss: 0.973560]\n",
      "419 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.998940] [MSE loss: 0.972333]\n",
      "429 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.998942] [MSE loss: 0.973065]\n",
      "439 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.998943] [MSE loss: 0.973062]\n",
      "449 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.998945] [MSE loss: 0.972701]\n",
      "459 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.998947] [MSE loss: 0.972056]\n",
      "469 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.998948] [MSE loss: 0.973246]\n",
      "479 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.998950] [MSE loss: 0.973087]\n",
      "489 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.998951] [MSE loss: 0.972892]\n",
      "499 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.998952] [MSE loss: 0.971039]\n",
      "509 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.998953] [MSE loss: 0.974138]\n",
      "519 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.998955] [MSE loss: 0.974084]\n",
      "529 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.998956] [MSE loss: 0.974207]\n",
      "539 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.998957] [MSE loss: 0.973531]\n",
      "549 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.998958] [MSE loss: 0.972651]\n",
      "559 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.998959] [MSE loss: 0.973258]\n",
      "569 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.998960] [MSE loss: 0.973913]\n",
      "579 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.998961] [MSE loss: 0.972560]\n",
      "589 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.998962] [MSE loss: 0.974092]\n",
      "599 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.998963] [MSE loss: 0.972631]\n",
      "609 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.998964] [MSE loss: 0.973221]\n",
      "619 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.998965] [MSE loss: 0.973443]\n",
      "629 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.998966] [MSE loss: 0.973477]\n",
      "639 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998966] [MSE loss: 0.972273]\n",
      "649 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998967] [MSE loss: 0.973414]\n",
      "659 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998968] [MSE loss: 0.974008]\n",
      "669 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998968] [MSE loss: 0.973239]\n",
      "679 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998969] [MSE loss: 0.970576]\n",
      "689 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.998970] [MSE loss: 0.973677]\n",
      "699 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998970] [MSE loss: 0.973959]\n",
      "709 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998971] [MSE loss: 0.973463]\n",
      "719 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998972] [MSE loss: 0.972768]\n",
      "729 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998972] [MSE loss: 0.971506]\n",
      "739 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998973] [MSE loss: 0.975815]\n",
      "749 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998973] [MSE loss: 0.973439]\n",
      "759 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.998974] [MSE loss: 0.971254]\n",
      "769 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998975] [MSE loss: 0.971973]\n",
      "779 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998975] [MSE loss: 0.973122]\n",
      "789 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998976] [MSE loss: 0.973708]\n",
      "799 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998976] [MSE loss: 0.972767]\n",
      "809 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998977] [MSE loss: 0.971907]\n",
      "819 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998977] [MSE loss: 0.972628]\n",
      "829 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998977] [MSE loss: 0.972819]\n",
      "839 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.998978] [MSE loss: 0.975257]\n",
      "849 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998978] [MSE loss: 0.973313]\n",
      "859 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998979] [MSE loss: 0.972268]\n",
      "869 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998979] [MSE loss: 0.974199]\n",
      "879 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998980] [MSE loss: 0.973519]\n",
      "889 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998980] [MSE loss: 0.974034]\n",
      "899 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998980] [MSE loss: 0.972129]\n",
      "909 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998981] [MSE loss: 0.972576]\n",
      "919 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998981] [MSE loss: 0.972515]\n",
      "929 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998981] [MSE loss: 0.973640]\n",
      "939 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.998982] [MSE loss: 0.972297]\n",
      "949 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998982] [MSE loss: 0.973970]\n",
      "959 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998982] [MSE loss: 0.972980]\n",
      "969 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998983] [MSE loss: 0.972493]\n",
      "979 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998983] [MSE loss: 0.974433]\n",
      "989 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998983] [MSE loss: 0.973047]\n",
      "999 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998984] [MSE loss: 0.972213]\n",
      "1009 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998984] [MSE loss: 0.972619]\n",
      "1019 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998984] [MSE loss: 0.973166]\n",
      "1029 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998985] [MSE loss: 0.973919]\n",
      "1039 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998985] [MSE loss: 0.973530]\n",
      "1049 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998985] [MSE loss: 0.971940]\n",
      "1059 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998985] [MSE loss: 0.973872]\n",
      "1069 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998985] [MSE loss: 0.972485]\n",
      "1079 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998986] [MSE loss: 0.974924]\n",
      "1089 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.998986] [MSE loss: 0.972739]\n",
      "1099 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998986] [MSE loss: 0.972426]\n",
      "1109 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998986] [MSE loss: 0.973069]\n",
      "1119 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998987] [MSE loss: 0.972904]\n",
      "1129 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998987] [MSE loss: 0.973204]\n",
      "1139 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998987] [MSE loss: 0.972346]\n",
      "1149 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998987] [MSE loss: 0.974960]\n",
      "1159 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998988] [MSE loss: 0.972123]\n",
      "1169 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998988] [MSE loss: 0.974429]\n",
      "1179 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998988] [MSE loss: 0.971203]\n",
      "1189 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998988] [MSE loss: 0.972412]\n",
      "1199 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998988] [MSE loss: 0.972552]\n",
      "1209 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.971822]\n",
      "1219 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.972517]\n",
      "1229 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.973227]\n",
      "1239 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.973022]\n",
      "1249 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.975156]\n",
      "1259 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998989] [MSE loss: 0.973925]\n",
      "1269 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973106]\n",
      "1279 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973852]\n",
      "1289 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973577]\n",
      "1299 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973403]\n",
      "1309 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973992]\n",
      "1319 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973124]\n",
      "1329 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998990] [MSE loss: 0.973010]\n",
      "1339 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.972382]\n",
      "1349 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.973292]\n",
      "1359 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.973990]\n",
      "1369 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.972953]\n",
      "1379 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.971883]\n",
      "1389 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.973919]\n",
      "1399 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998991] [MSE loss: 0.973438]\n",
      "1409 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.972185]\n",
      "1419 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.974002]\n",
      "1429 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.973149]\n",
      "1439 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.973287]\n",
      "1449 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.972725]\n",
      "1459 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.974539]\n",
      "1469 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.974597]\n",
      "1479 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998992] [MSE loss: 0.972138]\n",
      "1489 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.971597]\n",
      "1499 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.973864]\n",
      "1509 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.974783]\n",
      "1519 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.970807]\n",
      "1529 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.973233]\n",
      "1539 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.972442]\n",
      "1549 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.972378]\n",
      "1559 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.973211]\n",
      "1569 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.971922]\n",
      "1579 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998993] [MSE loss: 0.972606]\n",
      "1589 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.973844]\n",
      "1599 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972876]\n",
      "1609 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.973264]\n",
      "1619 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972328]\n",
      "1629 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.975161]\n",
      "1639 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972876]\n",
      "1649 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972371]\n",
      "1659 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972260]\n",
      "1669 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.972776]\n",
      "1679 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.971826]\n",
      "1689 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998994] [MSE loss: 0.971664]\n",
      "1699 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.974211]\n",
      "1709 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972714]\n",
      "1719 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.970755]\n",
      "1729 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.973920]\n",
      "1739 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972443]\n",
      "1749 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972014]\n",
      "1759 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972243]\n",
      "1769 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972344]\n",
      "1779 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972703]\n",
      "1789 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.973399]\n",
      "1799 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.971917]\n",
      "1809 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.971987]\n",
      "1819 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.972948]\n",
      "1829 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.973086]\n",
      "1839 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.973722]\n",
      "1849 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998995] [MSE loss: 0.973627]\n",
      "1859 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972520]\n",
      "1869 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.974936]\n",
      "1879 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973227]\n",
      "1889 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972507]\n",
      "1899 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973880]\n",
      "1909 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973386]\n",
      "1919 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972419]\n",
      "1929 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972468]\n",
      "1939 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973635]\n",
      "1949 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972560]\n",
      "1959 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972843]\n",
      "1969 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972607]\n",
      "1979 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973615]\n",
      "1989 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973285]\n",
      "1999 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.975806]\n",
      "2009 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.974818]\n",
      "2019 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.973101]\n",
      "2029 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998996] [MSE loss: 0.972848]\n",
      "2039 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973804]\n",
      "2049 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.971658]\n",
      "2059 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972184]\n",
      "2069 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972633]\n",
      "2079 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972791]\n",
      "2089 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.974161]\n",
      "2099 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973342]\n",
      "2109 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972206]\n",
      "2119 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973311]\n",
      "2129 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973927]\n",
      "2139 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.971898]\n",
      "2149 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973716]\n",
      "2159 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972621]\n",
      "2169 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973460]\n",
      "2179 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972401]\n",
      "2189 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.974233]\n",
      "2199 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972247]\n",
      "2209 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973577]\n",
      "2219 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972146]\n",
      "2229 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973954]\n",
      "2239 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.973357]\n",
      "2249 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.974079]\n",
      "2259 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972910]\n",
      "2269 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972160]\n",
      "2279 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998997] [MSE loss: 0.972325]\n",
      "2289 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998998] [MSE loss: 0.972667]\n",
      "2299 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998998] [MSE loss: 0.971614]\n",
      "2309 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998998] [MSE loss: 0.972606]\n",
      "2319 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998998] [MSE loss: 0.971907]\n",
      "2329 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998998] [MSE loss: 0.973892]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-545f97a695d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-545f97a695d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Train the generator (to have the discriminator label samples as valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Plot the progress every 20 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \"\"\"\n\u001b[1;32m   1636\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m   def train_on_batch(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3574\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3575\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3576\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3577\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    858\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 860\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    861\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m    143\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AssignVariableOp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[1;32m    145\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MIGAN():\n",
    "    def __init__(self):\n",
    "        self.frames = 3\n",
    "        self.img_rows = 64 #28\n",
    "        self.img_cols = 64 #28\n",
    "        self.channels = 1 #1\n",
    "        self.version = \"2.3\"\n",
    "        self.train_version = \"a\"\n",
    "        \n",
    "        # train as sequence with 3 frames\n",
    "        self.seq_shape = (self.frames, self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "\n",
    "        # Cut and load the dataset to shape (90000,3,64,64,1)\n",
    "        data = np.load(\"mnist_test_seq.npy\")\n",
    "        train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[8:14],data[14:20]),axis=1)\n",
    "        test_set = data[2:8]\n",
    "        \n",
    "        self.Y_train = np.expand_dims(train_set[3:6].transpose(1,0,2,3), axis=4)\n",
    "        self.X_train = np.expand_dims(train_set[0:3].transpose(1,0,2,3), axis=4)\n",
    "        \n",
    "        self.Y_test = np.expand_dims(test_set[3:6].transpose(1,0,2,3), axis=4)\n",
    "        self.X_test = np.expand_dims(test_set[0:3].transpose(1,0,2,3), axis=4)\n",
    "        \n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes ipt_imgs as input and generates gen_imgs\n",
    "        ipt_imgs = Input(shape=(self.seq_shape))\n",
    "        gen_imgs = self.generator(ipt_imgs)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(gen_imgs)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(ipt_imgs, validity)\n",
    "        self.combined.compile(loss=['mean_squared_error','binary_crossentropy'],loss_weights=[0.999,0.001],optimizer=optimizer)\n",
    "#         self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "        # Set checkpoints and save trained models\n",
    "        self.checkpoint_dir = 'training_checkpoints' + self.version\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n",
    "                                         discriminator_optimizer=optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "\n",
    "        model_convlstm = Sequential(\n",
    "            [\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, input_shape=self.seq_shape),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "            ])\n",
    "\n",
    "        model_convlstm.summary()\n",
    "        \n",
    "\n",
    "\n",
    "        ipt_imgs = Input(shape=(self.seq_shape))\n",
    "        gen_imgs = model_convlstm(ipt_imgs)\n",
    "\n",
    "        return Model(ipt_imgs, gen_imgs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.seq_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "\n",
    "        sam_imgs = Input(shape=self.seq_shape)\n",
    "        validity = model(sam_imgs)\n",
    "\n",
    "        return Model(sam_imgs, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size , sample_interval):\n",
    "        # prepare lists for storing stats each iteration\n",
    "        d1_hist, d2_hist, g_hist, mse_hist= list(), list(), list(), list()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size) \n",
    "            sam_imgs = Y_train[idx] #For Y_train\n",
    "            ipt_imgs = X_train[idx]\n",
    "            \n",
    "            # Generate a batch of new images            \n",
    "            gen_imgs = self.generator.predict(ipt_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(sam_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            MSE_loss = self.evaluate()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(ipt_imgs, valid)\n",
    "\n",
    "            # Plot the progress every 20 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [MSE loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, MSE_loss))\n",
    "            \n",
    "            # Save the models every 60 epochs\n",
    "            if (epoch + 1) % 60 == 0:\n",
    "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "                \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % 20 == 0:\n",
    "                self.save_images(epoch)\n",
    "                \n",
    "            # Record history\n",
    "            d1_hist.append(d_loss[0])\n",
    "            d2_hist.append(d_loss[1])\n",
    "            g_hist.append(g_loss)\n",
    "            mse_hist.append(MSE_loss)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                self.plot_history(d1_hist, d2_hist, g_hist, mse_hist, epoch)\n",
    "\n",
    "                \n",
    "\n",
    "    def save_images(self, epoch):\n",
    "        saveimg_dir = \"generated_images\"+self.version +\"/%s.png\"\n",
    "        save_name = self.train_version + str(epoch+480)\n",
    "        \n",
    "        # Select Y_train and X_train\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Select a clip for ploting\n",
    "        idx = np.random.randint(0, Y_train.shape[0], 32)\n",
    "        ipt_imgs = X_train[idx][0].squeeze()\n",
    "        gen_imgs = self.generator.predict(X_train[idx])[0].squeeze()\n",
    "        sam_imgs = Y_train[idx][0].squeeze()\n",
    "        \n",
    "        # Plot images\n",
    "        fig = plt.figure()\n",
    "        row1 = plt.subplot(3,3,1)\n",
    "        plt.imshow(sam_imgs[0,:,:], cmap='gray')\n",
    "        row1.title.set_text(\"Target sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,2)\n",
    "        plt.imshow(sam_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,3)\n",
    "        plt.imshow(sam_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        row2 = plt.subplot(3,3,4)\n",
    "        plt.imshow(ipt_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Input sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,5)\n",
    "        plt.imshow(ipt_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,6)\n",
    "        plt.imshow(ipt_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "                \n",
    "        row2 = plt.subplot(3,3,7)\n",
    "        plt.imshow(gen_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Generated sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,8)\n",
    "        plt.imshow(gen_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,9)\n",
    "        plt.imshow(gen_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        fig.savefig(saveimg_dir % save_name) #%d\n",
    "        plt.close()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "    def plot_history(self, d1_hist, d2_hist, g_hist, mse_hist, epoch):\n",
    "        saveimg_dir = \"generated_images\"+self.version +\"/%s.png\"\n",
    "        save_name = \"loss-\" + self.train_version + str(epoch+400) \n",
    "        \n",
    "        # plot loss\n",
    "        plt.plot(d1_hist, label='d-real')\n",
    "        plt.plot(d2_hist, label='d-fake')\n",
    "        plt.plot(g_hist, label='gen')\n",
    "        plt.plot(mse_hist, label='mse')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.savefig(saveimg_dir % save_name)\n",
    "        plt.close()\n",
    "\n",
    "#         plt.show\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        avg_mse = 0\n",
    "        Y_test = self.Y_test / 127.5 - 1.0\n",
    "        X_test = self.X_test / 127.5 - 1.0\n",
    "        \n",
    "        # Select a clip\n",
    "        idx = np.random.randint(0, Y_test.shape[0], 32)\n",
    "        ipt_sequences = X_test[idx].squeeze()\n",
    "        gen_sequences = self.generator.predict(X_test[idx]).squeeze()\n",
    "        tgt_sequences = Y_test[idx].squeeze()\n",
    "        \n",
    "        avg_mse += tf.reduce_mean(tf.math.squared_difference(tgt_sequences, gen_sequences)).numpy()\n",
    "#         print(avg_mse)\n",
    "        return avg_mse\n",
    "                            \n",
    "                                    \n",
    "if __name__ == '__main__':\n",
    "    gan = MIGAN()\n",
    "    gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\n",
    "    gan.train(epochs=10000, batch_size=32, sample_interval=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281.4463500976562"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"mnist_test_seq.npy\")\n",
    "all_mse = 0\n",
    "avg_mse = 0\n",
    "\n",
    "test_set = np.concatenate((data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "Y_test = test_set[3:6].transpose(1,0,2,3)\n",
    "X_test = test_set[0:3].transpose(1,0,2,3)\n",
    "\n",
    "# print(X_test[1][0])\n",
    "\n",
    "gen_img = X_test[0]\n",
    "sam_img = Y_test[0]\n",
    "\n",
    "gen_img = 0.5 * gen_img + 0.5\n",
    "sam_img = 0.5 * sam_img + 0.5\n",
    "        \n",
    "avg_mse = tf.reduce_mean(tf.math.squared_difference(sam_img, gen_img)).numpy()\n",
    "\n",
    "avg_mse\n",
    "\n",
    "# for i in range(10):\n",
    "#     input_sequence = X_test[i]\n",
    "#     target_sequence = Y_test[i]\n",
    "    \n",
    "#     all_mse += tf.losses.MSE(input_sequence[0],target_sequence[0])\n",
    "# avg_mse = all_msl / 10\n",
    "\n",
    "# avg_mse\n",
    "                              \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Past/dir2.2'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Past/\"\n",
    "b = \"dir\"\n",
    "c = \"2.2\"\n",
    "d = a + b + c\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
