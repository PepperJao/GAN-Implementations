{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify layers, maybe ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, ConvLSTM2D, Conv3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_36 (ConvLSTM2D) (None, 3, 64, 64, 40)     59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_37 (ConvLSTM2D) (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_38 (ConvLSTM2D) (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_39 (ConvLSTM2D) (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 3, 64, 64, 1)      1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3, 64, 64, 1) for input Tensor(\"input_39:0\", shape=(None, 3, 64, 64, 1), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3, 64, 64, 1) for input Tensor(\"input_37:0\", shape=(None, 3, 64, 64, 1), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3, 64, 64, 1) for input Tensor(\"flatten_9_input:0\", shape=(None, 3, 64, 64, 1), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3, 1).\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8de011c7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8de011c7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8e1f4477a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8e1f4477a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8e1f45e680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8e1f45e680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input Tensor(\"input_40:0\", shape=(None, 64, 64, 3), dtype=float32), but it was called on an input with incompatible shape (32, 3, 64, 64).\n",
      "59 [D loss: 0.732667, acc.: 0.00%] [G loss: 0.274437]\n",
      "119 [D loss: 0.710886, acc.: 0.00%] [G loss: 0.259289]\n",
      "179 [D loss: 0.704527, acc.: 1.56%] [G loss: 0.255414]\n",
      "239 [D loss: 0.702798, acc.: 1.56%] [G loss: 0.255283]\n",
      "299 [D loss: 1.130555, acc.: 0.00%] [G loss: 0.453207]\n",
      "359 [D loss: 0.875989, acc.: 0.00%] [G loss: 0.340706]\n",
      "419 [D loss: 0.808065, acc.: 0.00%] [G loss: 0.307776]\n",
      "479 [D loss: 0.774092, acc.: 0.00%] [G loss: 0.292739]\n",
      "539 [D loss: 0.757743, acc.: 0.00%] [G loss: 0.279110]\n",
      "599 [D loss: 0.741980, acc.: 0.00%] [G loss: 0.275137]\n",
      "659 [D loss: 0.732633, acc.: 0.00%] [G loss: 0.269786]\n",
      "719 [D loss: 0.725370, acc.: 0.00%] [G loss: 0.265218]\n",
      "779 [D loss: 0.718584, acc.: 0.00%] [G loss: 0.263679]\n",
      "839 [D loss: 0.753650, acc.: 0.00%] [G loss: 0.277806]\n",
      "899 [D loss: 0.720187, acc.: 0.00%] [G loss: 0.263621]\n",
      "959 [D loss: 0.710351, acc.: 1.56%] [G loss: 0.258105]\n",
      "1019 [D loss: 0.914379, acc.: 0.00%] [G loss: 0.354034]\n",
      "1079 [D loss: 0.787079, acc.: 0.00%] [G loss: 0.298539]\n",
      "1139 [D loss: 0.755143, acc.: 0.00%] [G loss: 0.281122]\n",
      "1199 [D loss: 0.733014, acc.: 0.00%] [G loss: 0.269194]\n",
      "1259 [D loss: 0.722779, acc.: 0.00%] [G loss: 0.265725]\n",
      "1319 [D loss: 0.735283, acc.: 0.00%] [G loss: 0.271691]\n",
      "1379 [D loss: 0.714148, acc.: 0.00%] [G loss: 0.260369]\n",
      "1439 [D loss: 0.706139, acc.: 0.00%] [G loss: 0.256857]\n",
      "1499 [D loss: 0.703240, acc.: 0.00%] [G loss: 0.255406]\n",
      "1559 [D loss: 0.947313, acc.: 0.00%] [G loss: 0.372558]\n",
      "1619 [D loss: 0.813141, acc.: 0.00%] [G loss: 0.308078]\n",
      "1679 [D loss: 0.767469, acc.: 0.00%] [G loss: 0.285427]\n",
      "1739 [D loss: 0.744852, acc.: 0.00%] [G loss: 0.274960]\n",
      "1799 [D loss: 0.729017, acc.: 0.00%] [G loss: 0.268596]\n",
      "1859 [D loss: 0.719782, acc.: 0.00%] [G loss: 0.263068]\n",
      "1919 [D loss: 0.712766, acc.: 0.00%] [G loss: 0.272417]\n",
      "1979 [D loss: 0.708939, acc.: 0.00%] [G loss: 0.257849]\n",
      "2039 [D loss: 0.725069, acc.: 0.00%] [G loss: 0.264695]\n",
      "2099 [D loss: 0.704676, acc.: 0.00%] [G loss: 0.255048]\n",
      "2159 [D loss: 0.758118, acc.: 15.62%] [G loss: 0.873126]\n",
      "2219 [D loss: 0.905701, acc.: 0.00%] [G loss: 0.352368]\n",
      "2279 [D loss: 0.810940, acc.: 0.00%] [G loss: 0.306843]\n",
      "2339 [D loss: 0.768708, acc.: 0.00%] [G loss: 0.289595]\n",
      "2399 [D loss: 0.749446, acc.: 0.00%] [G loss: 0.277314]\n",
      "2459 [D loss: 0.736191, acc.: 0.00%] [G loss: 0.269874]\n",
      "2519 [D loss: 0.724799, acc.: 0.00%] [G loss: 0.270924]\n",
      "2579 [D loss: 0.805429, acc.: 0.00%] [G loss: 0.305947]\n",
      "2639 [D loss: 0.755753, acc.: 0.00%] [G loss: 0.282215]\n",
      "2699 [D loss: 0.735154, acc.: 0.00%] [G loss: 0.271435]\n",
      "2759 [D loss: 0.722343, acc.: 0.00%] [G loss: 0.264090]\n",
      "2819 [D loss: 0.716852, acc.: 0.00%] [G loss: 0.261131]\n",
      "2879 [D loss: 0.707967, acc.: 0.00%] [G loss: 0.257208]\n",
      "2939 [D loss: 0.743786, acc.: 0.00%] [G loss: 0.265214]\n",
      "2999 [D loss: 0.701564, acc.: 0.00%] [G loss: 0.254335]\n",
      "3059 [D loss: 0.738914, acc.: 0.00%] [G loss: 0.272454]\n",
      "3119 [D loss: 0.713714, acc.: 0.00%] [G loss: 0.259754]\n",
      "3179 [D loss: 0.705197, acc.: 0.00%] [G loss: 0.255900]\n",
      "3239 [D loss: 0.974432, acc.: 0.00%] [G loss: 0.383439]\n",
      "3299 [D loss: 0.822409, acc.: 0.00%] [G loss: 0.313765]\n",
      "3359 [D loss: 0.775714, acc.: 0.00%] [G loss: 0.289174]\n",
      "3419 [D loss: 0.751061, acc.: 0.00%] [G loss: 0.278729]\n",
      "3479 [D loss: 0.736285, acc.: 0.00%] [G loss: 0.271205]\n",
      "3539 [D loss: 0.727636, acc.: 0.00%] [G loss: 0.268465]\n",
      "3599 [D loss: 0.718573, acc.: 0.00%] [G loss: 0.263103]\n",
      "3659 [D loss: 0.715734, acc.: 0.00%] [G loss: 0.261392]\n",
      "3719 [D loss: 0.709636, acc.: 0.00%] [G loss: 0.257695]\n",
      "3779 [D loss: 0.710823, acc.: 0.00%] [G loss: 0.258792]\n",
      "3839 [D loss: 0.705798, acc.: 0.00%] [G loss: 0.255134]\n",
      "3899 [D loss: 0.703764, acc.: 0.00%] [G loss: 0.254863]\n",
      "3959 [D loss: 1.217670, acc.: 0.00%] [G loss: 0.491723]\n",
      "4019 [D loss: 0.877370, acc.: 0.00%] [G loss: 0.342264]\n",
      "4079 [D loss: 0.807343, acc.: 0.00%] [G loss: 0.310363]\n",
      "4139 [D loss: 0.775174, acc.: 0.00%] [G loss: 0.293757]\n",
      "4199 [D loss: 0.758282, acc.: 0.00%] [G loss: 0.282872]\n",
      "4259 [D loss: 0.745790, acc.: 0.00%] [G loss: 0.276970]\n",
      "4319 [D loss: 0.733292, acc.: 0.00%] [G loss: 0.272503]\n",
      "4379 [D loss: 0.726577, acc.: 0.00%] [G loss: 0.268098]\n",
      "4439 [D loss: 0.720713, acc.: 0.00%] [G loss: 0.264090]\n",
      "4499 [D loss: 0.715507, acc.: 0.00%] [G loss: 0.261465]\n",
      "4559 [D loss: 0.712850, acc.: 0.00%] [G loss: 0.259633]\n",
      "4619 [D loss: 1.007164, acc.: 0.00%] [G loss: 0.403165]\n",
      "4679 [D loss: 0.808529, acc.: 0.00%] [G loss: 0.306271]\n",
      "4739 [D loss: 0.762722, acc.: 0.00%] [G loss: 0.284856]\n",
      "4799 [D loss: 0.743661, acc.: 0.00%] [G loss: 0.275177]\n",
      "4859 [D loss: 0.729482, acc.: 0.00%] [G loss: 0.269272]\n",
      "4919 [D loss: 0.722031, acc.: 0.00%] [G loss: 0.264555]\n",
      "4979 [D loss: 0.714934, acc.: 0.00%] [G loss: 0.261425]\n"
     ]
    }
   ],
   "source": [
    "class MIGAN():\n",
    "    def __init__(self):\n",
    "        self.frames = 3\n",
    "        self.img_rows = 64 #28\n",
    "        self.img_cols = 64 #28\n",
    "        self.channels = 1 #1\n",
    "        #train as image with 3 channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.frames) \n",
    "        #train as sequence with 3 frames\n",
    "        self.seq_shape = (self.frames, self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "\n",
    "        # Cut and load the dataset to shape (90000,3,64,64,1)\n",
    "        data = np.load(\"mnist_test_seq.npy\")\n",
    "        train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "        self.Y_train = np.expand_dims(train_set[3:6].transpose(1,0,2,3), axis=4)\n",
    "        self.X_train = np.expand_dims(train_set[0:3].transpose(1,0,2,3), axis=4)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "#         self.generator.compile(loss=['mean_squared_error','mean_absolute_error'],loss_weights=[0.001, 0.999],optimizer=optimizer)\n",
    "\n",
    "        # The generator takes ipt_imgs as input and generates gen_imgs\n",
    "        ipt_imgs = Input(shape=(self.img_shape))\n",
    "        gen_imgs = self.generator(ipt_imgs)\n",
    "        \n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False #True\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(gen_imgs)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(ipt_imgs, validity)\n",
    "        self.combined.compile(loss=['mean_squared_error'binary_crossentropy'],loss_weights=[0.999,0.001],optimizer=optimizer)\n",
    "#         self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "        # Set checkpoints and save trained models\n",
    "        self.checkpoint_dir = 'training_checkpoints2.2_G untrained'\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"convlstm\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n",
    "                                         discriminator_optimizer=optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model_convlstm = Sequential(\n",
    "            [\n",
    "                Input(shape=self.seq_shape),  # Variable-length sequence of 64x64x1 frames\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "            ])\n",
    "\n",
    "        model_convlstm.summary()\n",
    "        \n",
    "\n",
    "\n",
    "        ipt_imgs = Input(shape=(self.seq_shape))\n",
    "        gen_imgs = model(ipt_imgs)\n",
    "\n",
    "        return Model(ipt_imgs, gen_imgs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.seq_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        sam_imgs = Input(shape=self.seq_shape)\n",
    "        validity = model(sam_imgs)\n",
    "\n",
    "        return Model(sam_imgs, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=32 , sample_interval=30):\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size) \n",
    "            sam_imgs = Y_train[idx] #For Y_train\n",
    "            ipt_imgs = X_train[idx]\n",
    "            \n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(ipt_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(sam_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(ipt_imgs, valid)\n",
    "\n",
    "            # Plot the progress every 20 epochs\n",
    "            if (epoch + 1) % (3*sample_interval) == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # Save the models every 60 epochs\n",
    "            if (epoch + 1) % (3*sample_interval) == 0:\n",
    "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "                \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.save_images(epoch)\n",
    "\n",
    "\n",
    "    def save_images(self, epoch):\n",
    "\n",
    "        # Select Y_train and X_train\n",
    "        Y_train = self.Y_train\n",
    "        X_train = self.X_train\n",
    "        \n",
    "        # Select a clip for ploting\n",
    "        idx = np.random.randint(0, Y_train.shape[0], 32)\n",
    "        ipt_imgs = X_train[idx][0].squeeze()\n",
    "        gen_imgs = self.generator.predict(X_train[idx])[0].squeeze()\n",
    "        sam_imgs = Y_train[idx][0].squeeze()\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        sam_imgs = 0.5 * sam_imgs + 0.5\n",
    "        \n",
    "        # Plot images\n",
    "        fig = plt.figure()\n",
    "        row1 = plt.subplot(3,3,1)\n",
    "        plt.imshow(sam_imgs[0,:,:], cmap='gray')\n",
    "        row1.title.set_text(\"Target sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,2)\n",
    "        plt.imshow(sam_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,3)\n",
    "        plt.imshow(sam_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        row2 = plt.subplot(3,3,4)\n",
    "        plt.imshow(ipt_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Input sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,5)\n",
    "        plt.imshow(ipt_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,6)\n",
    "        plt.imshow(ipt_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "                \n",
    "        row2 = plt.subplot(3,3,7)\n",
    "        plt.imshow(gen_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Generated sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,8)\n",
    "        plt.imshow(gen_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,9)\n",
    "        plt.imshow(gen_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        crt_epoch = 'convlstm'+str(epoch)\n",
    "        fig.savefig(\"generated_images2.2_G untrained/%s.png\" % crt_epoch) #%d\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = MIGAN()\n",
    "    gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\n",
    "#     gan.save_images(\"test\")\n",
    "    gan.train(epochs=5000, batch_size=32, sample_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load(\"mnist_test_seq.npy\")\n",
    "train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "Y_train = np.expand_dims(train_set[3:6].transpose(1,0,2,3), axis=4)\n",
    "\n",
    "idx = np.random.randint(0, Y_train.shape[0], 10) \n",
    "sam_imgs = Y_train[idx][0].squeeze() #For Y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "row1 = plt.subplot(2,3,1)\n",
    "plt.imshow(sam_imgs[0,:,:], cmap='gray')\n",
    "row1.title.set_text(\"Real sequence\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(sam_imgs[1,:,:], cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(sam_imgs[2,:,:], cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10000, 64, 64)\n",
      "(6, 90000, 64, 64)\n",
      "(90000, 3, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"mnist_test_seq.npy\")\n",
    "print(data.shape)\n",
    "train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "print(train_set.shape)\n",
    "Y_train = np.expand_dims(train_set[3:6].transpose(1,0,2,3), axis=4)\n",
    "X_train = train_set[0:3].transpose(1,0,2,3)\n",
    "print(Y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12288)             6303744   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 12,861,696\n",
      "Trainable params: 12,860,160\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64, 64, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 128)       589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 667,137\n",
      "Trainable params: 666,753\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 3, 64, 64, 40)     59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 3, 64, 64, 1)      1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frames = 3\n",
    "img_rows = 64 #28\n",
    "img_cols = 64 #28\n",
    "channels = 1 #1\n",
    "seq_shape = (frames, img_rows, img_cols, channels)\n",
    "img_shape = (img_rows, img_cols, frames)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=seq_shape))\n",
    "model.add(Dense(512,input_shape=img_shape))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
    "model.add(Reshape(img_shape))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(512, activation=\"relu\", input_shape=img_shape))\n",
    "# model2.add(Reshape((7, 7, 128)))\n",
    "# model2.add(UpSampling2D())\n",
    "model2.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "model2.add(BatchNormalization(momentum=0.8))\n",
    "model2.add(Activation(\"relu\"))\n",
    "# model2.add(UpSampling2D())\n",
    "model2.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "model2.add(BatchNormalization(momentum=0.8))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "model2.add(Activation(\"tanh\"))\n",
    "\n",
    "\n",
    "model2.summary()\n",
    "        \n",
    "\n",
    "model_convlstm = Sequential(\n",
    "    [\n",
    "        Input(shape=(seq_shape)),  # Variable-length sequence of 64x64x1 frames\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "    ])\n",
    "\n",
    "model_convlstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_126 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_127 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frames = 3\n",
    "img_rows = 64 #28\n",
    "img_cols = 64 #28\n",
    "channels = 1 #1\n",
    "seq_shape = (frames, img_rows, img_cols, channels)\n",
    "img_shape = (img_rows, img_cols, frames)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=seq_shape))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_8 (ConvLSTM2D)  (None, 3, 64, 64, 40)     59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_9 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_10 (ConvLSTM2D) (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_11 (ConvLSTM2D) (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 3, 64, 64, 1)      1081      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 12289     \n",
      "=================================================================\n",
      "Total params: 419,290\n",
      "Trainable params: 418,970\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, ConvLSTM2D, Conv3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "frames = 3\n",
    "img_rows = 64 #28\n",
    "img_cols = 64 #28\n",
    "channels = 1 #1\n",
    "seq_shape = (frames, img_rows, img_cols, channels)\n",
    "img_shape = (img_rows, img_cols, frames)\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=seq_shape),  # Variable-length sequence of 64x64x1 frames\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "# model.add(Flatten(input_shape=self.seq_shape))\n",
    "# model.add(Dense(512))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(Dense(256))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
