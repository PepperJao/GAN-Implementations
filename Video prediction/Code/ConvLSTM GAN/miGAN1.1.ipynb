{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With training checkpoint\n",
    "# Simplify data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12288)             6303744   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 12,861,696\n",
      "Trainable params: 12,860,160\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f1ba5edfb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f1ba5edfb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1ba5df27a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1ba5df27a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1ba46dfdd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1ba46dfdd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "9 [D loss: 0.845604, acc.: 1.56%] [G loss: 0.862756]\n",
      "19 [D loss: 0.766214, acc.: 17.19%] [G loss: 0.855241]\n",
      "29 [D loss: 0.771809, acc.: 17.19%] [G loss: 0.801125]\n",
      "39 [D loss: 0.748745, acc.: 18.75%] [G loss: 0.801298]\n",
      "49 [D loss: 0.754927, acc.: 17.19%] [G loss: 0.837389]\n",
      "59 [D loss: 0.721161, acc.: 43.75%] [G loss: 0.869121]\n",
      "69 [D loss: 0.777269, acc.: 9.38%] [G loss: 0.839294]\n",
      "79 [D loss: 0.745417, acc.: 25.00%] [G loss: 0.827970]\n",
      "89 [D loss: 0.740290, acc.: 25.00%] [G loss: 0.801715]\n",
      "99 [D loss: 0.738760, acc.: 20.31%] [G loss: 0.813471]\n",
      "109 [D loss: 0.755152, acc.: 18.75%] [G loss: 0.829590]\n",
      "119 [D loss: 0.765590, acc.: 17.19%] [G loss: 0.770968]\n",
      "129 [D loss: 0.767920, acc.: 14.06%] [G loss: 0.809595]\n",
      "139 [D loss: 0.777465, acc.: 14.06%] [G loss: 0.848731]\n",
      "149 [D loss: 0.727382, acc.: 42.19%] [G loss: 0.830334]\n",
      "159 [D loss: 0.782422, acc.: 18.75%] [G loss: 0.889009]\n",
      "169 [D loss: 0.743465, acc.: 20.31%] [G loss: 0.932583]\n",
      "179 [D loss: 0.753086, acc.: 20.31%] [G loss: 0.809437]\n",
      "189 [D loss: 0.755784, acc.: 35.94%] [G loss: 0.820213]\n",
      "199 [D loss: 0.735871, acc.: 29.69%] [G loss: 0.813883]\n",
      "209 [D loss: 2.054780, acc.: 0.00%] [G loss: 2.055947]\n",
      "219 [D loss: 1.089441, acc.: 3.12%] [G loss: 1.088680]\n",
      "229 [D loss: 0.980025, acc.: 3.12%] [G loss: 0.964392]\n",
      "239 [D loss: 0.900854, acc.: 6.25%] [G loss: 0.847896]\n",
      "249 [D loss: 0.827351, acc.: 4.69%] [G loss: 0.900660]\n",
      "259 [D loss: 0.843353, acc.: 14.06%] [G loss: 0.882016]\n",
      "269 [D loss: 0.788078, acc.: 4.69%] [G loss: 0.827225]\n",
      "279 [D loss: 0.773550, acc.: 14.06%] [G loss: 0.846742]\n",
      "289 [D loss: 0.774935, acc.: 23.44%] [G loss: 0.799453]\n",
      "299 [D loss: 0.756207, acc.: 28.12%] [G loss: 0.813092]\n",
      "309 [D loss: 0.749338, acc.: 23.44%] [G loss: 0.788454]\n",
      "319 [D loss: 0.735767, acc.: 28.12%] [G loss: 0.823257]\n",
      "329 [D loss: 0.735644, acc.: 26.56%] [G loss: 0.828412]\n",
      "339 [D loss: 0.782202, acc.: 9.38%] [G loss: 0.786689]\n",
      "349 [D loss: 0.752713, acc.: 10.94%] [G loss: 0.823406]\n",
      "359 [D loss: 0.769957, acc.: 28.12%] [G loss: 0.827418]\n",
      "369 [D loss: 0.776583, acc.: 31.25%] [G loss: 0.889298]\n",
      "379 [D loss: 0.753031, acc.: 28.12%] [G loss: 0.825689]\n",
      "389 [D loss: 0.727186, acc.: 20.31%] [G loss: 0.812082]\n",
      "399 [D loss: 0.777427, acc.: 9.38%] [G loss: 0.783959]\n",
      "409 [D loss: 0.726855, acc.: 32.81%] [G loss: 0.810414]\n",
      "419 [D loss: 0.727897, acc.: 32.81%] [G loss: 0.821877]\n",
      "429 [D loss: 0.790079, acc.: 7.81%] [G loss: 0.787346]\n",
      "439 [D loss: 0.753049, acc.: 28.12%] [G loss: 0.827107]\n",
      "449 [D loss: 0.745721, acc.: 15.62%] [G loss: 0.784806]\n",
      "459 [D loss: 0.755815, acc.: 43.75%] [G loss: 0.828579]\n",
      "469 [D loss: 0.955383, acc.: 17.19%] [G loss: 0.889524]\n",
      "479 [D loss: 0.790392, acc.: 10.94%] [G loss: 0.842884]\n",
      "489 [D loss: 0.777972, acc.: 31.25%] [G loss: 0.802013]\n",
      "499 [D loss: 0.761148, acc.: 23.44%] [G loss: 0.812310]\n",
      "509 [D loss: 0.731974, acc.: 18.75%] [G loss: 0.811320]\n",
      "519 [D loss: 0.772467, acc.: 6.25%] [G loss: 0.805109]\n",
      "529 [D loss: 0.775425, acc.: 18.75%] [G loss: 0.821236]\n",
      "539 [D loss: 0.724632, acc.: 29.69%] [G loss: 0.792455]\n",
      "549 [D loss: 0.733672, acc.: 37.50%] [G loss: 0.795330]\n",
      "559 [D loss: 0.755682, acc.: 21.88%] [G loss: 0.748067]\n",
      "569 [D loss: 0.779192, acc.: 23.44%] [G loss: 0.787746]\n",
      "579 [D loss: 0.748495, acc.: 32.81%] [G loss: 0.808338]\n",
      "589 [D loss: 0.744957, acc.: 32.81%] [G loss: 0.813495]\n",
      "599 [D loss: 0.740156, acc.: 26.56%] [G loss: 0.785108]\n",
      "609 [D loss: 0.750693, acc.: 17.19%] [G loss: 0.799532]\n",
      "619 [D loss: 0.740925, acc.: 18.75%] [G loss: 0.787570]\n",
      "629 [D loss: 0.732393, acc.: 23.44%] [G loss: 0.784965]\n",
      "639 [D loss: 0.737152, acc.: 21.88%] [G loss: 0.915446]\n",
      "649 [D loss: 0.752330, acc.: 18.75%] [G loss: 0.780324]\n",
      "659 [D loss: 0.765657, acc.: 9.38%] [G loss: 0.801468]\n",
      "669 [D loss: 0.729551, acc.: 29.69%] [G loss: 0.819338]\n",
      "679 [D loss: 0.734607, acc.: 40.62%] [G loss: 0.826016]\n",
      "689 [D loss: 0.754417, acc.: 34.38%] [G loss: 0.803689]\n",
      "699 [D loss: 0.782036, acc.: 12.50%] [G loss: 0.844700]\n",
      "709 [D loss: 0.793578, acc.: 17.19%] [G loss: 0.815076]\n",
      "719 [D loss: 0.748937, acc.: 31.25%] [G loss: 0.821561]\n",
      "729 [D loss: 0.750773, acc.: 21.88%] [G loss: 0.814823]\n",
      "739 [D loss: 0.765900, acc.: 21.88%] [G loss: 0.802999]\n",
      "749 [D loss: 0.774507, acc.: 25.00%] [G loss: 0.827345]\n",
      "759 [D loss: 0.759862, acc.: 14.06%] [G loss: 0.800516]\n",
      "769 [D loss: 0.792551, acc.: 17.19%] [G loss: 0.818628]\n",
      "779 [D loss: 0.734707, acc.: 34.38%] [G loss: 0.793238]\n",
      "789 [D loss: 0.760649, acc.: 12.50%] [G loss: 0.856169]\n",
      "799 [D loss: 0.791290, acc.: 23.44%] [G loss: 0.923495]\n",
      "809 [D loss: 0.736884, acc.: 25.00%] [G loss: 0.806457]\n",
      "819 [D loss: 0.735916, acc.: 29.69%] [G loss: 0.809119]\n",
      "829 [D loss: 0.744801, acc.: 32.81%] [G loss: 0.805841]\n",
      "839 [D loss: 0.777653, acc.: 7.81%] [G loss: 0.804900]\n",
      "849 [D loss: 0.750655, acc.: 31.25%] [G loss: 0.811387]\n",
      "859 [D loss: 0.777132, acc.: 28.12%] [G loss: 0.813912]\n",
      "869 [D loss: 0.715404, acc.: 46.88%] [G loss: 0.788332]\n",
      "879 [D loss: 0.756430, acc.: 20.31%] [G loss: 0.855427]\n",
      "889 [D loss: 0.759381, acc.: 26.56%] [G loss: 0.840655]\n",
      "899 [D loss: 0.747500, acc.: 23.44%] [G loss: 0.832718]\n",
      "909 [D loss: 0.756516, acc.: 21.88%] [G loss: 0.852653]\n",
      "919 [D loss: 0.767319, acc.: 17.19%] [G loss: 0.794794]\n",
      "929 [D loss: 0.783856, acc.: 23.44%] [G loss: 0.808490]\n",
      "939 [D loss: 1.558273, acc.: 0.00%] [G loss: 1.201370]\n",
      "949 [D loss: 0.963827, acc.: 4.69%] [G loss: 1.047266]\n",
      "959 [D loss: 0.845849, acc.: 6.25%] [G loss: 0.954854]\n",
      "969 [D loss: 0.804896, acc.: 21.88%] [G loss: 0.817320]\n",
      "979 [D loss: 0.774635, acc.: 15.62%] [G loss: 0.825517]\n",
      "989 [D loss: 0.788346, acc.: 28.12%] [G loss: 0.864114]\n",
      "999 [D loss: 0.763743, acc.: 23.44%] [G loss: 0.827651]\n",
      "1009 [D loss: 0.721954, acc.: 39.06%] [G loss: 0.831872]\n",
      "1019 [D loss: 0.725595, acc.: 35.94%] [G loss: 0.835918]\n",
      "1029 [D loss: 0.736328, acc.: 40.62%] [G loss: 0.799151]\n",
      "1039 [D loss: 0.783144, acc.: 35.94%] [G loss: 0.790411]\n",
      "1049 [D loss: 0.730493, acc.: 39.06%] [G loss: 0.809406]\n",
      "1059 [D loss: 0.766167, acc.: 32.81%] [G loss: 0.790011]\n",
      "1069 [D loss: 0.726937, acc.: 34.38%] [G loss: 0.783945]\n",
      "1079 [D loss: 0.712143, acc.: 34.38%] [G loss: 0.806468]\n",
      "1089 [D loss: 0.743717, acc.: 20.31%] [G loss: 0.807948]\n",
      "1099 [D loss: 0.739013, acc.: 37.50%] [G loss: 0.813436]\n",
      "1109 [D loss: 0.749900, acc.: 18.75%] [G loss: 0.818482]\n",
      "1119 [D loss: 0.746284, acc.: 18.75%] [G loss: 0.796127]\n",
      "1129 [D loss: 0.733756, acc.: 31.25%] [G loss: 0.826073]\n",
      "1139 [D loss: 0.739657, acc.: 23.44%] [G loss: 0.863650]\n",
      "1149 [D loss: 0.736894, acc.: 20.31%] [G loss: 0.790841]\n",
      "1159 [D loss: 0.719443, acc.: 37.50%] [G loss: 0.827333]\n",
      "1169 [D loss: 0.748732, acc.: 28.12%] [G loss: 0.827295]\n",
      "1179 [D loss: 0.774522, acc.: 28.12%] [G loss: 0.814856]\n",
      "1189 [D loss: 0.740336, acc.: 28.12%] [G loss: 0.818638]\n",
      "1199 [D loss: 0.726625, acc.: 46.88%] [G loss: 0.807422]\n",
      "1209 [D loss: 0.717316, acc.: 35.94%] [G loss: 0.775048]\n",
      "1219 [D loss: 0.732521, acc.: 32.81%] [G loss: 0.816911]\n",
      "1229 [D loss: 0.753841, acc.: 17.19%] [G loss: 0.845347]\n",
      "1239 [D loss: 0.758371, acc.: 23.44%] [G loss: 0.791246]\n",
      "1249 [D loss: 0.738749, acc.: 28.12%] [G loss: 0.830349]\n",
      "1259 [D loss: 0.792586, acc.: 18.75%] [G loss: 0.842141]\n",
      "1269 [D loss: 0.780514, acc.: 12.50%] [G loss: 0.831447]\n",
      "1279 [D loss: 0.736818, acc.: 31.25%] [G loss: 0.825943]\n",
      "1289 [D loss: 0.754480, acc.: 26.56%] [G loss: 0.810961]\n",
      "1299 [D loss: 0.830933, acc.: 3.12%] [G loss: 0.852437]\n",
      "1309 [D loss: 0.768479, acc.: 26.56%] [G loss: 0.773964]\n",
      "1319 [D loss: 0.729987, acc.: 31.25%] [G loss: 0.817268]\n",
      "1329 [D loss: 0.750764, acc.: 18.75%] [G loss: 0.778219]\n",
      "1339 [D loss: 0.759873, acc.: 23.44%] [G loss: 0.840278]\n",
      "1349 [D loss: 0.758118, acc.: 29.69%] [G loss: 0.814591]\n",
      "1359 [D loss: 0.766617, acc.: 26.56%] [G loss: 0.816034]\n",
      "1369 [D loss: 0.740869, acc.: 28.12%] [G loss: 0.813948]\n",
      "1379 [D loss: 0.747504, acc.: 23.44%] [G loss: 0.785399]\n",
      "1389 [D loss: 0.735310, acc.: 34.38%] [G loss: 0.823061]\n",
      "1399 [D loss: 0.885041, acc.: 0.00%] [G loss: 0.906176]\n",
      "1409 [D loss: 0.793399, acc.: 12.50%] [G loss: 0.823150]\n",
      "1419 [D loss: 0.832788, acc.: 18.75%] [G loss: 0.803567]\n",
      "1429 [D loss: 0.731441, acc.: 29.69%] [G loss: 0.832424]\n"
     ]
    }
   ],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64 #28\n",
    "        self.img_cols = 64 #28\n",
    "        self.channels = 3 #1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Cut and load the dataset to shape (60000,64,64,3)\n",
    "        data = np.load(\"mnist_test_seq.npy\")\n",
    "        self.Y_train = np.concatenate((data[0:3],data[3:6],data[6:9],data[9:12],data[12:15],data[15:18]),axis=1).transpose(1,2,3,0)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes sta_imgs as input and generates gen_imgs\n",
    "        sta_imgs = Input(shape=(self.img_shape))\n",
    "        gen_imgs = self.generator(sta_imgs)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(gen_imgs)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(sta_imgs, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        # Set checkpoints and save trained models\n",
    "        self.checkpoint_dir = 'training_checkpoints'\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n",
    "                                         discriminator_optimizer=optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        sta_imgs = Input(shape=(self.img_shape))\n",
    "        gen_imgs = model(sta_imgs)\n",
    "\n",
    "        return Model(sta_imgs, gen_imgs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        sam_imgs = Input(shape=self.img_shape)\n",
    "        validity = model(sam_imgs)\n",
    "\n",
    "        return Model(sam_imgs, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=32 , sample_interval=30):\n",
    "\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        Y_train = self.Y_train / 127.5 - 1.\n",
    "        \n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size) \n",
    "            sam_imgs = Y_train[idx] #For Y_train\n",
    "            \n",
    "            X_train = np.expand_dims(Y_train[:,:,:,0], axis=3) #For X_train\n",
    "            X_train = np.concatenate((X_train,X_train,X_train),axis=3)\n",
    "            sta_imgs = X_train[idx]\n",
    "            \n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(sta_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(sam_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(sta_imgs, valid)\n",
    "\n",
    "            # Plot the progress every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # Save the models every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "                \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "\n",
    "        # Select Y_train and X_train\n",
    "        Y_train = self.Y_train\n",
    "        X_train = np.expand_dims(Y_train[:,:,:,0], axis=3) \n",
    "        X_train = np.concatenate((X_train,X_train,X_train),axis=3)\n",
    "        \n",
    "        # Select a clip for ploting\n",
    "        idx = np.random.randint(0, Y_train.shape[0], 32)\n",
    "        sta_imgs = X_train[idx]\n",
    "        gen_imgs = self.generator.predict(sta_imgs)\n",
    "        sam_imgs = Y_train[idx]\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        sam_imgs = 0.5 * sam_imgs + 0.5\n",
    "        \n",
    "        # Plot images\n",
    "        fig = plt.figure()\n",
    "        row1 = plt.subplot(2,3,1)\n",
    "        plt.imshow(sam_imgs[0,:,:,0], cmap='gray')\n",
    "        row1.title.set_text(\"Real sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2,3,2)\n",
    "        plt.imshow(sam_imgs[0,:,:,1], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2,3,3)\n",
    "        plt.imshow(sam_imgs[0,:,:,2], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        row2 = plt.subplot(2,3,4)\n",
    "        plt.imshow(gen_imgs[0,:,:,0], cmap='gray')\n",
    "        row2.title.set_text(\"Generated sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2,3,5)\n",
    "        plt.imshow(gen_imgs[0,:,:,1], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2,3,6)\n",
    "        plt.imshow(gen_imgs[0,:,:,2], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        epoch = str(int(epoch)+7380)\n",
    "        fig.savefig(\"generated_images/%s.png\" % epoch) #%d\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\n",
    "#     gan.sample_images(\"test\")\n",
    "    gan.train(epochs=50000, batch_size=32, sample_interval=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
