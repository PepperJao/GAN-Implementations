{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input 123, output 234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, ConvLSTM2D, Conv3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 3, 64, 64, 40)     59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 3, 64, 64, 40)     115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 64, 64, 40)     160       \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 3, 64, 64, 1)      1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f7984229950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f7984229950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f798414d320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f798414d320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7b20970dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7b20970dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "9 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.128634]\n",
      "19 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.126447]\n",
      "29 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.123673]\n",
      "39 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.109328]\n",
      "49 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.102614]\n",
      "59 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.108641]\n",
      "69 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.097550]\n",
      "79 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.106081]\n",
      "89 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.085152]\n",
      "99 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.093108]\n",
      "109 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.076523]\n",
      "119 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.072637]\n",
      "129 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.066631]\n",
      "139 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.062684]\n",
      "149 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.053830]\n",
      "159 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.052433]\n",
      "169 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.046913]\n",
      "179 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.040098]\n",
      "189 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.027672]\n",
      "199 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.017245]\n",
      "209 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.018941]\n",
      "219 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.015410]\n",
      "229 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.007893]\n",
      "239 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.995127]\n",
      "249 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.979118]\n",
      "259 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.976631]\n",
      "269 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.970597]\n",
      "279 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.951287]\n",
      "289 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.946161]\n",
      "299 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.934826]\n",
      "309 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.924603]\n",
      "319 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.917541]\n",
      "329 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.906001]\n",
      "339 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.892029]\n",
      "349 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.886186]\n",
      "359 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.876078]\n",
      "369 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.877273]\n",
      "379 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.860409]\n",
      "389 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.858139]\n",
      "399 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.855264]\n",
      "409 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.848367]\n",
      "419 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.849039]\n",
      "429 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.842398]\n",
      "439 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.837785]\n",
      "449 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.844551]\n",
      "459 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.841640]\n",
      "469 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.844689]\n",
      "479 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.853045]\n",
      "489 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.854438]\n",
      "499 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.867196]\n",
      "509 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.872497]\n",
      "519 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.878768]\n",
      "529 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.887074]\n",
      "539 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.902990]\n",
      "549 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.914576]\n",
      "559 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.924400]\n",
      "569 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.935659]\n",
      "579 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.950936]\n",
      "589 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.967124]\n",
      "599 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.966309]\n",
      "609 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 1.989747]\n",
      "619 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.001207]\n",
      "629 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.014116]\n",
      "639 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.022796]\n",
      "649 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.041998]\n",
      "659 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.052469]\n",
      "669 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.064111]\n",
      "679 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.082276]\n",
      "689 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.096111]\n",
      "699 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.100597]\n",
      "709 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.109659]\n",
      "719 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.126447]\n",
      "729 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000] [MSE loss: 2.135272]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd9d958de6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;31m#     gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bd9d958de6fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mMSE_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bd9d958de6fc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mavg_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Select a clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MIGAN():\n",
    "    def __init__(self):\n",
    "        self.frames = 3\n",
    "        self.img_rows = 64 #28\n",
    "        self.img_cols = 64 #28\n",
    "        self.channels = 1 #1\n",
    "        self.version = \"2.5\"\n",
    "        self.train_version = \"shifted+mse+basicD_\"\n",
    "        \n",
    "        # train as sequence with 3 frames\n",
    "        self.seq_shape = (self.frames, self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "\n",
    "        # Cut and load the dataset to shape (90000,3,64,64,1)\n",
    "        data = np.load(\"mnist_test_seq.npy\")\n",
    "        train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[8:14],data[14:20]),axis=1)\n",
    "        test_set = data[2:8]\n",
    "        \n",
    "        self.Y_train = np.expand_dims(train_set[1:4].transpose(1,0,2,3), axis=4)\n",
    "        self.X_train = np.expand_dims(train_set[0:3].transpose(1,0,2,3), axis=4)\n",
    "        \n",
    "        self.Y_test = np.expand_dims(test_set[1:4].transpose(1,0,2,3), axis=4)\n",
    "        self.X_test = np.expand_dims(test_set[0:3].transpose(1,0,2,3), axis=4)\n",
    "        \n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes ipt_imgs as input and generates gen_imgs\n",
    "        ipt_imgs = Input(shape=(self.seq_shape))\n",
    "        gen_imgs = self.generator(ipt_imgs)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(gen_imgs)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(ipt_imgs, validity)\n",
    "        self.combined.compile(loss=['mean_squared_error','binary_crossentropy'],loss_weights=[0.999,0.001],optimizer=optimizer)\n",
    "#         self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "        # Set checkpoints and save trained models\n",
    "        self.checkpoint_dir = 'training_checkpoints' + self.version\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, self.train_version)\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n",
    "                                         discriminator_optimizer=optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "\n",
    "        model_convlstm = Sequential(\n",
    "            [\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, input_shape=self.seq_shape),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                ConvLSTM2D(filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True),\n",
    "                BatchNormalization(),\n",
    "                Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "            ])\n",
    "\n",
    "        model_convlstm.summary()\n",
    "        \n",
    "\n",
    "\n",
    "        ipt_imgs = Input(shape=(self.seq_shape))\n",
    "        gen_imgs = model_convlstm(ipt_imgs)\n",
    "\n",
    "        return Model(ipt_imgs, gen_imgs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.seq_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "\n",
    "        sam_imgs = Input(shape=self.seq_shape)\n",
    "        validity = model(sam_imgs)\n",
    "\n",
    "        return Model(sam_imgs, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size , sample_interval):\n",
    "        # prepare lists for storing stats each iteration\n",
    "        d1_hist, d2_hist, g_hist, mse_hist= list(), list(), list(), list()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size) \n",
    "            sam_imgs = Y_train[idx] #For Y_train\n",
    "            ipt_imgs = X_train[idx]\n",
    "            \n",
    "            # Generate a batch of new images            \n",
    "            gen_imgs = self.generator.predict(ipt_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(sam_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            MSE_loss = self.evaluate()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(ipt_imgs, valid)\n",
    "\n",
    "            # Plot the progress every 20 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [MSE loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, MSE_loss))\n",
    "            \n",
    "            # Save the models every 60 epochs\n",
    "            if (epoch + 1) % 60 == 0:\n",
    "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "                \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % 20 == 0:\n",
    "                self.save_images(epoch)\n",
    "                \n",
    "            # Record history\n",
    "            d1_hist.append(d_loss[0])\n",
    "            d2_hist.append(d_loss[1])\n",
    "            g_hist.append(g_loss)\n",
    "            mse_hist.append(MSE_loss)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                self.plot_history(d1_hist, d2_hist, g_hist, mse_hist, epoch)\n",
    "\n",
    "                \n",
    "\n",
    "    def save_images(self, epoch):\n",
    "        saveimg_dir = \"generated_images\"+self.version +\"/%s.png\"\n",
    "        save_name = self.train_version + str(epoch)\n",
    "        \n",
    "        # Select Y_train and X_train\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Select a clip for ploting\n",
    "        idx = np.random.randint(0, Y_train.shape[0], 32)\n",
    "        ipt_imgs = X_train[idx][0].squeeze()\n",
    "        gen_imgs = self.generator.predict(X_train[idx])[0].squeeze()\n",
    "        sam_imgs = Y_train[idx][0].squeeze()\n",
    "        \n",
    "        # Plot images\n",
    "        fig = plt.figure()\n",
    "        row1 = plt.subplot(3,3,1)\n",
    "        plt.imshow(sam_imgs[0,:,:], cmap='gray')\n",
    "        row1.title.set_text(\"Target sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,2)\n",
    "        plt.imshow(sam_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,3)\n",
    "        plt.imshow(sam_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        row2 = plt.subplot(3,3,4)\n",
    "        plt.imshow(ipt_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Input sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,5)\n",
    "        plt.imshow(ipt_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,6)\n",
    "        plt.imshow(ipt_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "                \n",
    "        row2 = plt.subplot(3,3,7)\n",
    "        plt.imshow(gen_imgs[0,:,:], cmap='gray')\n",
    "        row2.title.set_text(\"Generated sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,8)\n",
    "        plt.imshow(gen_imgs[1,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,9)\n",
    "        plt.imshow(gen_imgs[2,:,:], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        fig.savefig(saveimg_dir % save_name) #%d\n",
    "        plt.close()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "    def plot_history(self, d1_hist, d2_hist, g_hist, mse_hist, epoch):\n",
    "        saveimg_dir = \"generated_images\"+self.version +\"/%s.png\"\n",
    "        save_name = \"loss\" + self.train_version + str(epoch+120) \n",
    "        \n",
    "        # plot loss\n",
    "        plt.plot(d1_hist, label='d-real')\n",
    "        plt.plot(d2_hist, label='d-fake')\n",
    "        plt.plot(g_hist, label='gen')\n",
    "        plt.plot(mse_hist, label='mse')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.savefig(saveimg_dir % save_name)\n",
    "        plt.close()\n",
    "\n",
    "#         plt.show\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        avg_mse = 0\n",
    "        Y_test = self.Y_test / 127.5 - 1.0\n",
    "        X_test = self.X_test / 127.5 - 1.0\n",
    "        \n",
    "        # Select a clip\n",
    "        idx = np.random.randint(0, Y_test.shape[0], 32)\n",
    "        ipt_sequences = X_test[idx].squeeze()\n",
    "        gen_sequences = self.generator.predict(X_test[idx]).squeeze()\n",
    "        tgt_sequences = Y_test[idx].squeeze()\n",
    "        \n",
    "        avg_mse += tf.reduce_mean(tf.math.squared_difference(tgt_sequences, gen_sequences)).numpy()\n",
    "#         print(avg_mse)\n",
    "        return avg_mse\n",
    "                            \n",
    "                                    \n",
    "if __name__ == '__main__':\n",
    "    gan = MIGAN()\n",
    "    gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\n",
    "    gan.train(epochs=10000, batch_size=32, sample_interval=20)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
