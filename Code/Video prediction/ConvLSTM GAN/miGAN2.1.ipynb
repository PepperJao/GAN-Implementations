{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Generator loss function \"mae\" failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,423,553\n",
      "Trainable params: 6,423,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12288)             6303744   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 12,861,696\n",
      "Trainable params: 12,860,160\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe8324109e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe8324109e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff3d9a2b7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff3d9a2b7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe832410170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe832410170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "19 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.003858]\n",
      "39 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.263901]\n",
      "59 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.399726]\n",
      "79 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.991231]\n",
      "99 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.998994]\n",
      "119 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.999000]\n",
      "139 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.999000]\n",
      "159 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.999000]\n",
      "179 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.999000]\n",
      "199 [D loss: 0.004294, acc.: 100.00%] [G loss: 0.999000]\n",
      "219 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.999000]\n",
      "239 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.999000]\n",
      "259 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.998999]\n",
      "279 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.999000]\n",
      "299 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.998468]\n",
      "319 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.998664]\n",
      "339 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.998678]\n",
      "359 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.998999]\n",
      "379 [D loss: 0.004013, acc.: 100.00%] [G loss: 0.999000]\n",
      "399 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.998653]\n",
      "419 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.998939]\n",
      "439 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.999000]\n",
      "459 [D loss: 0.009057, acc.: 100.00%] [G loss: 0.998985]\n",
      "479 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.998775]\n",
      "499 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.999000]\n",
      "519 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.999000]\n",
      "539 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.958384]\n",
      "559 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.496621]\n",
      "579 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.447040]\n",
      "599 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.899100]\n",
      "619 [D loss: 11.658476, acc.: 25.00%] [G loss: 0.699301]\n",
      "639 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.998921]\n",
      "659 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.998876]\n",
      "679 [D loss: 0.267537, acc.: 95.00%] [G loss: 0.936624]\n",
      "699 [D loss: 2.370505, acc.: 85.00%] [G loss: 0.659841]\n",
      "719 [D loss: 6.223023, acc.: 20.00%] [G loss: 0.679241]\n",
      "739 [D loss: 3.248236, acc.: 35.00%] [G loss: 0.596968]\n",
      "759 [D loss: 3.654013, acc.: 50.00%] [G loss: 0.651168]\n",
      "779 [D loss: 1.577455, acc.: 40.00%] [G loss: 0.604394]\n",
      "799 [D loss: 2.860222, acc.: 20.00%] [G loss: 0.399994]\n",
      "819 [D loss: 0.230210, acc.: 90.00%] [G loss: 0.644270]\n",
      "839 [D loss: 0.530570, acc.: 80.00%] [G loss: 0.575899]\n",
      "859 [D loss: 1.363420, acc.: 20.00%] [G loss: 0.639959]\n",
      "879 [D loss: 1.113665, acc.: 20.00%] [G loss: 0.592597]\n",
      "899 [D loss: 0.531701, acc.: 80.00%] [G loss: 0.612216]\n",
      "919 [D loss: 0.812988, acc.: 65.00%] [G loss: 0.730532]\n",
      "939 [D loss: 1.047602, acc.: 20.00%] [G loss: 0.402518]\n",
      "959 [D loss: 0.955331, acc.: 20.00%] [G loss: 0.545362]\n",
      "979 [D loss: 1.221283, acc.: 5.00%] [G loss: 0.686845]\n",
      "999 [D loss: 0.292998, acc.: 85.00%] [G loss: 0.542599]\n",
      "1019 [D loss: 0.326963, acc.: 80.00%] [G loss: 0.553762]\n",
      "1039 [D loss: 1.044075, acc.: 25.00%] [G loss: 0.546138]\n",
      "1059 [D loss: 0.490296, acc.: 80.00%] [G loss: 0.589990]\n",
      "1079 [D loss: 0.938129, acc.: 15.00%] [G loss: 0.518111]\n",
      "1099 [D loss: 0.740751, acc.: 65.00%] [G loss: 0.541202]\n",
      "1119 [D loss: 0.737342, acc.: 65.00%] [G loss: 0.522979]\n",
      "1139 [D loss: 1.008644, acc.: 0.00%] [G loss: 0.732428]\n",
      "1159 [D loss: 0.863319, acc.: 55.00%] [G loss: 0.563373]\n",
      "1179 [D loss: 0.724665, acc.: 40.00%] [G loss: 0.449868]\n",
      "1199 [D loss: 0.744071, acc.: 50.00%] [G loss: 0.472796]\n",
      "1219 [D loss: 1.056378, acc.: 25.00%] [G loss: 0.422511]\n",
      "1239 [D loss: 0.587281, acc.: 70.00%] [G loss: 0.534442]\n",
      "1259 [D loss: 0.774770, acc.: 55.00%] [G loss: 0.643876]\n",
      "1279 [D loss: 0.740089, acc.: 65.00%] [G loss: 0.518467]\n",
      "1299 [D loss: 0.552557, acc.: 80.00%] [G loss: 0.378159]\n",
      "1319 [D loss: 0.596113, acc.: 65.00%] [G loss: 0.377775]\n",
      "1339 [D loss: 0.429791, acc.: 85.00%] [G loss: 0.469992]\n",
      "1359 [D loss: 0.465761, acc.: 80.00%] [G loss: 0.532034]\n",
      "1379 [D loss: 0.709202, acc.: 65.00%] [G loss: 0.504324]\n",
      "1399 [D loss: 0.656484, acc.: 65.00%] [G loss: 0.465908]\n",
      "1419 [D loss: 1.938483, acc.: 20.00%] [G loss: 0.551432]\n",
      "1439 [D loss: 0.931962, acc.: 60.00%] [G loss: 0.454707]\n",
      "1459 [D loss: 0.914311, acc.: 65.00%] [G loss: 0.426737]\n",
      "1479 [D loss: 0.643831, acc.: 70.00%] [G loss: 0.299054]\n",
      "1499 [D loss: 0.851961, acc.: 55.00%] [G loss: 0.524445]\n",
      "1519 [D loss: 0.683870, acc.: 70.00%] [G loss: 0.494142]\n",
      "1539 [D loss: 0.671770, acc.: 65.00%] [G loss: 0.491421]\n",
      "1559 [D loss: 0.634507, acc.: 70.00%] [G loss: 0.418267]\n",
      "1579 [D loss: 0.732693, acc.: 60.00%] [G loss: 0.391066]\n",
      "1599 [D loss: 0.639982, acc.: 65.00%] [G loss: 0.607052]\n",
      "1619 [D loss: 0.509594, acc.: 75.00%] [G loss: 0.343847]\n",
      "1639 [D loss: 0.653919, acc.: 60.00%] [G loss: 0.657570]\n",
      "1659 [D loss: 0.546285, acc.: 75.00%] [G loss: 0.521478]\n",
      "1679 [D loss: 0.500356, acc.: 75.00%] [G loss: 0.503867]\n",
      "1699 [D loss: 0.579638, acc.: 70.00%] [G loss: 0.519725]\n",
      "1719 [D loss: 0.667411, acc.: 60.00%] [G loss: 0.392623]\n",
      "1739 [D loss: 0.779596, acc.: 55.00%] [G loss: 0.343102]\n",
      "1759 [D loss: 0.498650, acc.: 75.00%] [G loss: 0.471070]\n",
      "1779 [D loss: 0.539379, acc.: 75.00%] [G loss: 0.349790]\n",
      "1799 [D loss: 0.543658, acc.: 70.00%] [G loss: 0.484537]\n",
      "1819 [D loss: 0.712487, acc.: 15.00%] [G loss: 0.507873]\n",
      "1839 [D loss: 0.446595, acc.: 80.00%] [G loss: 0.510554]\n",
      "1859 [D loss: 0.687446, acc.: 25.00%] [G loss: 0.590645]\n",
      "1879 [D loss: 0.539059, acc.: 70.00%] [G loss: 0.494022]\n",
      "1899 [D loss: 0.643355, acc.: 60.00%] [G loss: 0.371830]\n",
      "1919 [D loss: 0.583825, acc.: 65.00%] [G loss: 0.459571]\n",
      "1939 [D loss: 0.422384, acc.: 85.00%] [G loss: 0.565702]\n",
      "1959 [D loss: 0.602654, acc.: 60.00%] [G loss: 0.700532]\n",
      "1979 [D loss: 0.679877, acc.: 30.00%] [G loss: 0.392142]\n",
      "1999 [D loss: 0.582642, acc.: 65.00%] [G loss: 0.319451]\n",
      "2019 [D loss: 0.622852, acc.: 50.00%] [G loss: 0.401585]\n",
      "2039 [D loss: 0.610186, acc.: 60.00%] [G loss: 0.472965]\n",
      "2059 [D loss: 0.703243, acc.: 45.00%] [G loss: 0.424621]\n",
      "2079 [D loss: 0.500772, acc.: 70.00%] [G loss: 0.545562]\n",
      "2099 [D loss: 0.581195, acc.: 70.00%] [G loss: 0.523672]\n",
      "2119 [D loss: 0.623846, acc.: 40.00%] [G loss: 0.547967]\n",
      "2139 [D loss: 0.924960, acc.: 30.00%] [G loss: 0.463319]\n",
      "2159 [D loss: 0.714701, acc.: 60.00%] [G loss: 0.412739]\n",
      "2179 [D loss: 0.700308, acc.: 60.00%] [G loss: 0.457957]\n",
      "2199 [D loss: 0.693498, acc.: 55.00%] [G loss: 0.484666]\n",
      "2219 [D loss: 0.681857, acc.: 50.00%] [G loss: 0.463469]\n",
      "2239 [D loss: 0.633994, acc.: 60.00%] [G loss: 0.446624]\n",
      "2259 [D loss: 0.741664, acc.: 45.00%] [G loss: 0.398776]\n",
      "2279 [D loss: 0.583493, acc.: 65.00%] [G loss: 0.559656]\n",
      "2299 [D loss: 0.685091, acc.: 50.00%] [G loss: 0.443403]\n",
      "2319 [D loss: 0.556782, acc.: 55.00%] [G loss: 0.500631]\n",
      "2339 [D loss: 0.522957, acc.: 55.00%] [G loss: 0.310418]\n",
      "2359 [D loss: 0.559137, acc.: 40.00%] [G loss: 0.482384]\n",
      "2379 [D loss: 0.511316, acc.: 65.00%] [G loss: 0.499821]\n",
      "2399 [D loss: 0.517450, acc.: 70.00%] [G loss: 0.591766]\n",
      "2419 [D loss: 0.465689, acc.: 75.00%] [G loss: 0.630403]\n",
      "2439 [D loss: 0.832663, acc.: 55.00%] [G loss: 0.526622]\n",
      "2459 [D loss: 0.637705, acc.: 60.00%] [G loss: 0.472218]\n",
      "2479 [D loss: 0.653579, acc.: 45.00%] [G loss: 0.626023]\n",
      "2499 [D loss: 0.887419, acc.: 35.00%] [G loss: 0.642270]\n",
      "2519 [D loss: 0.895053, acc.: 45.00%] [G loss: 0.365969]\n",
      "2539 [D loss: 0.865504, acc.: 40.00%] [G loss: 0.569620]\n",
      "2559 [D loss: 0.738150, acc.: 45.00%] [G loss: 0.613707]\n",
      "2579 [D loss: 0.670809, acc.: 60.00%] [G loss: 0.461229]\n",
      "2599 [D loss: 0.633957, acc.: 40.00%] [G loss: 0.615732]\n",
      "2619 [D loss: 0.576873, acc.: 65.00%] [G loss: 0.483383]\n",
      "2639 [D loss: 0.504305, acc.: 70.00%] [G loss: 0.407538]\n",
      "2659 [D loss: 0.871216, acc.: 40.00%] [G loss: 0.527517]\n",
      "2679 [D loss: 0.798453, acc.: 50.00%] [G loss: 0.419714]\n",
      "2699 [D loss: 0.634767, acc.: 65.00%] [G loss: 0.551116]\n",
      "2719 [D loss: 0.788966, acc.: 50.00%] [G loss: 0.480795]\n",
      "2739 [D loss: 0.573221, acc.: 60.00%] [G loss: 0.446397]\n",
      "2759 [D loss: 0.601513, acc.: 50.00%] [G loss: 0.485210]\n",
      "2779 [D loss: 0.498376, acc.: 75.00%] [G loss: 0.580039]\n",
      "2799 [D loss: 0.908309, acc.: 40.00%] [G loss: 0.634214]\n",
      "2819 [D loss: 0.546885, acc.: 55.00%] [G loss: 0.693329]\n",
      "2839 [D loss: 0.691764, acc.: 45.00%] [G loss: 0.535593]\n",
      "2859 [D loss: 0.741444, acc.: 50.00%] [G loss: 0.477049]\n",
      "2879 [D loss: 0.382904, acc.: 90.00%] [G loss: 0.556546]\n",
      "2899 [D loss: 0.641064, acc.: 65.00%] [G loss: 0.583668]\n",
      "2919 [D loss: 0.635995, acc.: 50.00%] [G loss: 0.510855]\n",
      "2939 [D loss: 0.825578, acc.: 40.00%] [G loss: 0.493308]\n",
      "2959 [D loss: 0.613859, acc.: 60.00%] [G loss: 0.486483]\n",
      "2979 [D loss: 0.851525, acc.: 50.00%] [G loss: 0.488479]\n",
      "2999 [D loss: 0.647976, acc.: 55.00%] [G loss: 0.451031]\n",
      "3019 [D loss: 0.789266, acc.: 50.00%] [G loss: 0.540981]\n",
      "3039 [D loss: 0.584889, acc.: 55.00%] [G loss: 0.558383]\n",
      "3059 [D loss: 0.710513, acc.: 40.00%] [G loss: 0.655420]\n",
      "3079 [D loss: 0.591835, acc.: 60.00%] [G loss: 0.448811]\n",
      "3099 [D loss: 0.824195, acc.: 40.00%] [G loss: 0.478094]\n",
      "3119 [D loss: 0.827857, acc.: 50.00%] [G loss: 0.499514]\n",
      "3139 [D loss: 0.594820, acc.: 60.00%] [G loss: 0.469219]\n",
      "3159 [D loss: 0.564491, acc.: 85.00%] [G loss: 0.534202]\n",
      "3179 [D loss: 0.751891, acc.: 40.00%] [G loss: 0.412434]\n",
      "3199 [D loss: 0.713093, acc.: 45.00%] [G loss: 0.490030]\n",
      "3219 [D loss: 0.674014, acc.: 55.00%] [G loss: 0.440636]\n",
      "3239 [D loss: 0.797067, acc.: 55.00%] [G loss: 0.606606]\n",
      "3259 [D loss: 0.985743, acc.: 30.00%] [G loss: 0.500772]\n",
      "3279 [D loss: 0.704302, acc.: 45.00%] [G loss: 0.481395]\n",
      "3299 [D loss: 0.535651, acc.: 60.00%] [G loss: 0.456607]\n",
      "3319 [D loss: 0.631783, acc.: 50.00%] [G loss: 0.486908]\n",
      "3339 [D loss: 0.823129, acc.: 45.00%] [G loss: 0.639442]\n",
      "3359 [D loss: 0.591763, acc.: 60.00%] [G loss: 0.529381]\n",
      "3379 [D loss: 0.599827, acc.: 65.00%] [G loss: 0.600207]\n",
      "3399 [D loss: 0.761221, acc.: 50.00%] [G loss: 0.497213]\n",
      "3419 [D loss: 0.383927, acc.: 90.00%] [G loss: 0.670196]\n",
      "3439 [D loss: 0.308111, acc.: 90.00%] [G loss: 0.446695]\n",
      "3459 [D loss: 0.707675, acc.: 50.00%] [G loss: 0.570321]\n",
      "3479 [D loss: 0.868912, acc.: 45.00%] [G loss: 0.449733]\n",
      "3499 [D loss: 0.750080, acc.: 45.00%] [G loss: 0.574835]\n",
      "3519 [D loss: 0.734446, acc.: 45.00%] [G loss: 0.443073]\n",
      "3539 [D loss: 0.696057, acc.: 55.00%] [G loss: 0.442311]\n",
      "3559 [D loss: 0.670520, acc.: 45.00%] [G loss: 0.528448]\n",
      "3579 [D loss: 0.754820, acc.: 25.00%] [G loss: 0.483777]\n",
      "3599 [D loss: 0.451209, acc.: 75.00%] [G loss: 0.435984]\n",
      "3619 [D loss: 0.916322, acc.: 35.00%] [G loss: 0.573326]\n",
      "3639 [D loss: 0.627067, acc.: 45.00%] [G loss: 0.485427]\n",
      "3659 [D loss: 0.714471, acc.: 50.00%] [G loss: 0.471528]\n",
      "3679 [D loss: 0.582227, acc.: 75.00%] [G loss: 0.537838]\n",
      "3699 [D loss: 0.522041, acc.: 75.00%] [G loss: 0.603145]\n",
      "3719 [D loss: 0.659920, acc.: 45.00%] [G loss: 0.325515]\n",
      "3739 [D loss: 0.631562, acc.: 55.00%] [G loss: 0.359511]\n",
      "3759 [D loss: 0.659120, acc.: 55.00%] [G loss: 0.400843]\n",
      "3779 [D loss: 0.679076, acc.: 50.00%] [G loss: 0.370835]\n",
      "3799 [D loss: 0.782115, acc.: 35.00%] [G loss: 0.315911]\n",
      "3819 [D loss: 0.665240, acc.: 55.00%] [G loss: 0.365803]\n",
      "3839 [D loss: 0.875141, acc.: 45.00%] [G loss: 0.339302]\n",
      "3859 [D loss: 0.663054, acc.: 60.00%] [G loss: 0.450952]\n",
      "3879 [D loss: 0.619301, acc.: 55.00%] [G loss: 0.493322]\n",
      "3899 [D loss: 0.667135, acc.: 50.00%] [G loss: 0.447999]\n",
      "3919 [D loss: 0.744968, acc.: 40.00%] [G loss: 0.488106]\n",
      "3939 [D loss: 0.715601, acc.: 55.00%] [G loss: 0.539983]\n",
      "3959 [D loss: 0.765977, acc.: 45.00%] [G loss: 0.477192]\n",
      "3979 [D loss: 0.503397, acc.: 70.00%] [G loss: 0.446949]\n",
      "3999 [D loss: 0.609447, acc.: 60.00%] [G loss: 0.588794]\n",
      "4019 [D loss: 0.750036, acc.: 45.00%] [G loss: 0.461536]\n",
      "4039 [D loss: 0.815296, acc.: 50.00%] [G loss: 0.448051]\n",
      "4059 [D loss: 0.598152, acc.: 60.00%] [G loss: 0.385707]\n",
      "4079 [D loss: 0.601676, acc.: 45.00%] [G loss: 0.380455]\n",
      "4099 [D loss: 0.921989, acc.: 35.00%] [G loss: 0.524279]\n",
      "4119 [D loss: 0.676915, acc.: 45.00%] [G loss: 0.434096]\n",
      "4139 [D loss: 0.692087, acc.: 50.00%] [G loss: 0.331182]\n",
      "4159 [D loss: 0.744099, acc.: 45.00%] [G loss: 0.273802]\n",
      "4179 [D loss: 0.640142, acc.: 55.00%] [G loss: 0.560401]\n",
      "4199 [D loss: 0.656080, acc.: 50.00%] [G loss: 0.465284]\n",
      "4219 [D loss: 0.803362, acc.: 70.00%] [G loss: 0.705699]\n",
      "4239 [D loss: 0.759372, acc.: 45.00%] [G loss: 0.554987]\n",
      "4259 [D loss: 0.754742, acc.: 50.00%] [G loss: 0.571648]\n",
      "4279 [D loss: 0.666616, acc.: 45.00%] [G loss: 0.380967]\n",
      "4299 [D loss: 0.961326, acc.: 30.00%] [G loss: 0.368506]\n",
      "4319 [D loss: 0.694701, acc.: 55.00%] [G loss: 0.408594]\n",
      "4339 [D loss: 0.653334, acc.: 60.00%] [G loss: 0.606284]\n",
      "4359 [D loss: 0.805770, acc.: 45.00%] [G loss: 0.505333]\n",
      "4379 [D loss: 0.670831, acc.: 65.00%] [G loss: 0.754198]\n",
      "4399 [D loss: 0.647339, acc.: 70.00%] [G loss: 0.462862]\n",
      "4419 [D loss: 0.644723, acc.: 60.00%] [G loss: 0.336492]\n",
      "4439 [D loss: 0.710512, acc.: 45.00%] [G loss: 0.364618]\n",
      "4459 [D loss: 0.681666, acc.: 40.00%] [G loss: 0.431054]\n",
      "4479 [D loss: 1.048726, acc.: 30.00%] [G loss: 0.365851]\n",
      "4499 [D loss: 0.739080, acc.: 40.00%] [G loss: 0.375864]\n",
      "4519 [D loss: 0.942468, acc.: 50.00%] [G loss: 0.538552]\n",
      "4539 [D loss: 0.889994, acc.: 35.00%] [G loss: 0.423719]\n",
      "4559 [D loss: 0.798448, acc.: 45.00%] [G loss: 0.654524]\n",
      "4579 [D loss: 0.663714, acc.: 55.00%] [G loss: 0.444380]\n",
      "4599 [D loss: 0.541584, acc.: 70.00%] [G loss: 0.474549]\n",
      "4619 [D loss: 0.927196, acc.: 40.00%] [G loss: 0.703715]\n",
      "4639 [D loss: 0.980948, acc.: 40.00%] [G loss: 0.395881]\n",
      "4659 [D loss: 0.606916, acc.: 55.00%] [G loss: 0.410940]\n",
      "4679 [D loss: 1.197520, acc.: 20.00%] [G loss: 0.301744]\n",
      "4699 [D loss: 0.734867, acc.: 55.00%] [G loss: 0.539059]\n",
      "4719 [D loss: 0.776003, acc.: 35.00%] [G loss: 0.463536]\n",
      "4739 [D loss: 0.926300, acc.: 40.00%] [G loss: 0.648183]\n",
      "4759 [D loss: 0.506085, acc.: 75.00%] [G loss: 0.418285]\n",
      "4779 [D loss: 0.794225, acc.: 35.00%] [G loss: 0.355609]\n",
      "4799 [D loss: 0.738993, acc.: 60.00%] [G loss: 0.537784]\n",
      "4819 [D loss: 0.842340, acc.: 30.00%] [G loss: 0.680213]\n",
      "4839 [D loss: 0.829601, acc.: 25.00%] [G loss: 0.523102]\n",
      "4859 [D loss: 0.837377, acc.: 40.00%] [G loss: 0.434542]\n",
      "4879 [D loss: 0.918065, acc.: 35.00%] [G loss: 0.470294]\n",
      "4899 [D loss: 0.621779, acc.: 55.00%] [G loss: 0.441009]\n",
      "4919 [D loss: 0.502769, acc.: 60.00%] [G loss: 0.375336]\n",
      "4939 [D loss: 0.849179, acc.: 20.00%] [G loss: 0.606428]\n",
      "4959 [D loss: 0.571017, acc.: 60.00%] [G loss: 0.509096]\n",
      "4979 [D loss: 0.711819, acc.: 40.00%] [G loss: 0.444700]\n",
      "4999 [D loss: 0.536271, acc.: 60.00%] [G loss: 0.398017]\n",
      "5019 [D loss: 0.665670, acc.: 50.00%] [G loss: 0.437582]\n",
      "5039 [D loss: 0.855406, acc.: 45.00%] [G loss: 0.436714]\n",
      "5059 [D loss: 0.746680, acc.: 45.00%] [G loss: 0.464892]\n",
      "5079 [D loss: 0.558936, acc.: 75.00%] [G loss: 0.544350]\n",
      "5099 [D loss: 0.532951, acc.: 60.00%] [G loss: 0.384467]\n",
      "5119 [D loss: 0.666925, acc.: 45.00%] [G loss: 0.526268]\n",
      "5139 [D loss: 0.873229, acc.: 35.00%] [G loss: 0.490476]\n",
      "5159 [D loss: 0.668606, acc.: 65.00%] [G loss: 0.462073]\n",
      "5179 [D loss: 0.605248, acc.: 70.00%] [G loss: 0.444190]\n",
      "5199 [D loss: 0.892460, acc.: 60.00%] [G loss: 0.439363]\n",
      "5219 [D loss: 0.675964, acc.: 65.00%] [G loss: 0.547716]\n",
      "5239 [D loss: 0.725748, acc.: 50.00%] [G loss: 0.544958]\n",
      "5259 [D loss: 0.803125, acc.: 40.00%] [G loss: 0.513684]\n",
      "5279 [D loss: 0.671447, acc.: 50.00%] [G loss: 0.431917]\n",
      "5299 [D loss: 0.651506, acc.: 45.00%] [G loss: 0.438876]\n",
      "5319 [D loss: 0.467979, acc.: 85.00%] [G loss: 0.433615]\n",
      "5339 [D loss: 0.631073, acc.: 75.00%] [G loss: 0.318177]\n",
      "5359 [D loss: 0.794639, acc.: 30.00%] [G loss: 0.453478]\n",
      "5379 [D loss: 0.563538, acc.: 65.00%] [G loss: 0.423705]\n",
      "5399 [D loss: 0.501877, acc.: 60.00%] [G loss: 0.486898]\n",
      "5419 [D loss: 0.685646, acc.: 55.00%] [G loss: 0.502442]\n",
      "5439 [D loss: 0.704725, acc.: 40.00%] [G loss: 0.517647]\n",
      "5459 [D loss: 0.378525, acc.: 90.00%] [G loss: 0.581480]\n",
      "5479 [D loss: 0.722081, acc.: 50.00%] [G loss: 0.480140]\n",
      "5499 [D loss: 0.841058, acc.: 40.00%] [G loss: 0.350113]\n",
      "5519 [D loss: 0.480968, acc.: 70.00%] [G loss: 0.598823]\n",
      "5539 [D loss: 0.621514, acc.: 55.00%] [G loss: 0.397434]\n",
      "5559 [D loss: 0.794311, acc.: 40.00%] [G loss: 0.482142]\n",
      "5579 [D loss: 0.818978, acc.: 45.00%] [G loss: 0.380213]\n",
      "5599 [D loss: 0.594340, acc.: 60.00%] [G loss: 0.490728]\n",
      "5619 [D loss: 0.772604, acc.: 40.00%] [G loss: 0.519646]\n",
      "5639 [D loss: 0.661775, acc.: 60.00%] [G loss: 0.438100]\n",
      "5659 [D loss: 0.595430, acc.: 60.00%] [G loss: 0.430121]\n",
      "5679 [D loss: 0.825671, acc.: 40.00%] [G loss: 0.490824]\n",
      "5699 [D loss: 0.368535, acc.: 85.00%] [G loss: 0.308648]\n",
      "5719 [D loss: 0.635489, acc.: 75.00%] [G loss: 0.391947]\n",
      "5739 [D loss: 0.681276, acc.: 45.00%] [G loss: 0.511250]\n",
      "5759 [D loss: 0.826726, acc.: 40.00%] [G loss: 0.483586]\n",
      "5779 [D loss: 0.662749, acc.: 65.00%] [G loss: 0.365690]\n",
      "5799 [D loss: 0.605307, acc.: 70.00%] [G loss: 0.367529]\n",
      "5819 [D loss: 0.615448, acc.: 60.00%] [G loss: 0.424394]\n",
      "5839 [D loss: 1.208393, acc.: 40.00%] [G loss: 0.413562]\n",
      "5859 [D loss: 0.643087, acc.: 60.00%] [G loss: 0.366110]\n",
      "5879 [D loss: 0.764719, acc.: 40.00%] [G loss: 0.513197]\n",
      "5899 [D loss: 0.767900, acc.: 45.00%] [G loss: 0.489639]\n",
      "5919 [D loss: 0.839540, acc.: 30.00%] [G loss: 0.581818]\n",
      "5939 [D loss: 0.702697, acc.: 40.00%] [G loss: 0.514595]\n",
      "5959 [D loss: 0.705899, acc.: 55.00%] [G loss: 0.475602]\n",
      "5979 [D loss: 0.716034, acc.: 50.00%] [G loss: 0.449989]\n",
      "5999 [D loss: 0.649348, acc.: 70.00%] [G loss: 0.418778]\n",
      "6019 [D loss: 0.796590, acc.: 25.00%] [G loss: 0.435666]\n",
      "6039 [D loss: 0.821205, acc.: 35.00%] [G loss: 0.393421]\n",
      "6059 [D loss: 0.743636, acc.: 40.00%] [G loss: 0.298775]\n",
      "6079 [D loss: 0.710338, acc.: 50.00%] [G loss: 0.433754]\n",
      "6099 [D loss: 0.612614, acc.: 55.00%] [G loss: 0.431829]\n",
      "6119 [D loss: 0.942150, acc.: 40.00%] [G loss: 0.541513]\n",
      "6139 [D loss: 0.709765, acc.: 65.00%] [G loss: 0.390162]\n",
      "6159 [D loss: 0.702686, acc.: 60.00%] [G loss: 0.502767]\n",
      "6179 [D loss: 0.707660, acc.: 50.00%] [G loss: 0.351163]\n",
      "6199 [D loss: 0.713629, acc.: 50.00%] [G loss: 0.377133]\n",
      "6219 [D loss: 0.713124, acc.: 40.00%] [G loss: 0.390627]\n",
      "6239 [D loss: 0.771385, acc.: 45.00%] [G loss: 0.413858]\n",
      "6259 [D loss: 0.651701, acc.: 45.00%] [G loss: 0.406634]\n",
      "6279 [D loss: 0.735195, acc.: 40.00%] [G loss: 0.392418]\n",
      "6299 [D loss: 0.656653, acc.: 50.00%] [G loss: 0.423232]\n",
      "6319 [D loss: 0.611237, acc.: 65.00%] [G loss: 0.415289]\n",
      "6339 [D loss: 0.617054, acc.: 65.00%] [G loss: 0.474452]\n",
      "6359 [D loss: 0.613846, acc.: 60.00%] [G loss: 0.418430]\n",
      "6379 [D loss: 0.575262, acc.: 70.00%] [G loss: 0.493174]\n",
      "6399 [D loss: 0.765683, acc.: 40.00%] [G loss: 0.386071]\n",
      "6419 [D loss: 0.681341, acc.: 45.00%] [G loss: 0.441356]\n",
      "6439 [D loss: 0.658422, acc.: 50.00%] [G loss: 0.390941]\n",
      "6459 [D loss: 0.740574, acc.: 35.00%] [G loss: 0.375511]\n",
      "6479 [D loss: 0.824061, acc.: 30.00%] [G loss: 0.405024]\n",
      "6499 [D loss: 0.714885, acc.: 45.00%] [G loss: 0.366837]\n",
      "6519 [D loss: 0.700593, acc.: 55.00%] [G loss: 0.445704]\n",
      "6539 [D loss: 0.848602, acc.: 20.00%] [G loss: 0.547905]\n",
      "6559 [D loss: 0.735769, acc.: 50.00%] [G loss: 0.547891]\n",
      "6579 [D loss: 0.667841, acc.: 55.00%] [G loss: 0.442160]\n",
      "6599 [D loss: 0.794738, acc.: 35.00%] [G loss: 0.471422]\n",
      "6619 [D loss: 0.703911, acc.: 45.00%] [G loss: 0.311507]\n",
      "6639 [D loss: 0.950749, acc.: 20.00%] [G loss: 0.286945]\n",
      "6659 [D loss: 0.589178, acc.: 65.00%] [G loss: 0.424571]\n",
      "6679 [D loss: 0.786065, acc.: 40.00%] [G loss: 0.337269]\n",
      "6699 [D loss: 0.687579, acc.: 50.00%] [G loss: 0.411472]\n",
      "6719 [D loss: 0.632390, acc.: 55.00%] [G loss: 0.521901]\n",
      "6739 [D loss: 0.556402, acc.: 65.00%] [G loss: 0.410170]\n",
      "6759 [D loss: 0.732482, acc.: 50.00%] [G loss: 0.518803]\n",
      "6779 [D loss: 0.594585, acc.: 75.00%] [G loss: 0.521292]\n",
      "6799 [D loss: 0.779337, acc.: 25.00%] [G loss: 0.470119]\n",
      "6819 [D loss: 0.647997, acc.: 45.00%] [G loss: 0.442746]\n",
      "6839 [D loss: 0.812798, acc.: 30.00%] [G loss: 0.567412]\n",
      "6859 [D loss: 0.841807, acc.: 55.00%] [G loss: 0.434209]\n",
      "6879 [D loss: 0.752054, acc.: 50.00%] [G loss: 0.381509]\n",
      "6899 [D loss: 0.707201, acc.: 35.00%] [G loss: 0.477312]\n",
      "6919 [D loss: 0.569611, acc.: 55.00%] [G loss: 0.435267]\n",
      "6939 [D loss: 0.654299, acc.: 60.00%] [G loss: 0.382818]\n",
      "6959 [D loss: 0.645309, acc.: 50.00%] [G loss: 0.382384]\n",
      "6979 [D loss: 0.760761, acc.: 35.00%] [G loss: 0.367298]\n",
      "6999 [D loss: 0.646486, acc.: 60.00%] [G loss: 0.394181]\n",
      "7019 [D loss: 0.567653, acc.: 75.00%] [G loss: 0.363462]\n",
      "7039 [D loss: 0.637919, acc.: 70.00%] [G loss: 0.427712]\n",
      "7059 [D loss: 0.670614, acc.: 45.00%] [G loss: 0.357070]\n",
      "7079 [D loss: 0.862137, acc.: 25.00%] [G loss: 0.390639]\n",
      "7099 [D loss: 0.747760, acc.: 45.00%] [G loss: 0.433196]\n",
      "7119 [D loss: 0.622522, acc.: 50.00%] [G loss: 0.349787]\n",
      "7139 [D loss: 0.822914, acc.: 45.00%] [G loss: 0.480738]\n",
      "7159 [D loss: 0.672452, acc.: 60.00%] [G loss: 0.372921]\n",
      "7179 [D loss: 0.788464, acc.: 35.00%] [G loss: 0.401273]\n",
      "7199 [D loss: 0.846662, acc.: 40.00%] [G loss: 0.545399]\n",
      "7219 [D loss: 0.681945, acc.: 45.00%] [G loss: 0.454731]\n",
      "7239 [D loss: 0.477862, acc.: 85.00%] [G loss: 0.373391]\n",
      "7259 [D loss: 0.719351, acc.: 25.00%] [G loss: 0.365303]\n",
      "7279 [D loss: 0.659250, acc.: 60.00%] [G loss: 0.378624]\n",
      "7299 [D loss: 0.740375, acc.: 40.00%] [G loss: 0.424146]\n",
      "7319 [D loss: 0.854998, acc.: 25.00%] [G loss: 0.375276]\n",
      "7339 [D loss: 0.586719, acc.: 50.00%] [G loss: 0.303297]\n",
      "7359 [D loss: 0.676504, acc.: 55.00%] [G loss: 0.443835]\n",
      "7379 [D loss: 0.630885, acc.: 65.00%] [G loss: 0.381728]\n",
      "7399 [D loss: 0.707011, acc.: 55.00%] [G loss: 0.386969]\n",
      "7419 [D loss: 0.656775, acc.: 50.00%] [G loss: 0.368289]\n",
      "7439 [D loss: 0.700325, acc.: 40.00%] [G loss: 0.389640]\n",
      "7459 [D loss: 0.684514, acc.: 40.00%] [G loss: 0.354347]\n",
      "7479 [D loss: 0.692446, acc.: 50.00%] [G loss: 0.406127]\n",
      "7499 [D loss: 0.683497, acc.: 50.00%] [G loss: 0.346206]\n",
      "7519 [D loss: 0.694776, acc.: 45.00%] [G loss: 0.395813]\n",
      "7539 [D loss: 0.849449, acc.: 30.00%] [G loss: 0.413045]\n",
      "7559 [D loss: 0.705995, acc.: 50.00%] [G loss: 0.352769]\n",
      "7579 [D loss: 0.676713, acc.: 50.00%] [G loss: 0.304352]\n",
      "7599 [D loss: 0.946112, acc.: 40.00%] [G loss: 0.442609]\n",
      "7619 [D loss: 0.750324, acc.: 30.00%] [G loss: 0.418846]\n",
      "7639 [D loss: 0.767919, acc.: 45.00%] [G loss: 0.226621]\n",
      "7659 [D loss: 0.791144, acc.: 45.00%] [G loss: 0.366435]\n",
      "7679 [D loss: 0.634992, acc.: 50.00%] [G loss: 0.280557]\n",
      "7699 [D loss: 0.718562, acc.: 55.00%] [G loss: 0.351070]\n",
      "7719 [D loss: 0.715907, acc.: 50.00%] [G loss: 0.539451]\n",
      "7739 [D loss: 0.653200, acc.: 55.00%] [G loss: 0.315422]\n",
      "7759 [D loss: 0.733260, acc.: 50.00%] [G loss: 0.379038]\n",
      "7779 [D loss: 0.717578, acc.: 45.00%] [G loss: 0.298171]\n",
      "7799 [D loss: 0.846186, acc.: 20.00%] [G loss: 0.396315]\n",
      "7819 [D loss: 1.119424, acc.: 25.00%] [G loss: 0.315201]\n",
      "7839 [D loss: 1.048131, acc.: 15.00%] [G loss: 0.281826]\n",
      "7859 [D loss: 0.756731, acc.: 50.00%] [G loss: 0.350677]\n",
      "7879 [D loss: 0.821528, acc.: 30.00%] [G loss: 0.364689]\n",
      "7899 [D loss: 0.681669, acc.: 60.00%] [G loss: 0.303667]\n",
      "7919 [D loss: 0.673035, acc.: 75.00%] [G loss: 0.378016]\n",
      "7939 [D loss: 0.611068, acc.: 50.00%] [G loss: 0.315148]\n",
      "7959 [D loss: 0.772200, acc.: 40.00%] [G loss: 0.394988]\n",
      "7979 [D loss: 0.579104, acc.: 75.00%] [G loss: 0.441142]\n",
      "7999 [D loss: 0.838577, acc.: 30.00%] [G loss: 0.322042]\n",
      "8019 [D loss: 0.738956, acc.: 40.00%] [G loss: 0.203584]\n",
      "8039 [D loss: 0.799687, acc.: 40.00%] [G loss: 0.339073]\n",
      "8059 [D loss: 0.734908, acc.: 50.00%] [G loss: 0.409729]\n",
      "8079 [D loss: 0.748864, acc.: 40.00%] [G loss: 0.289223]\n",
      "8099 [D loss: 0.719323, acc.: 45.00%] [G loss: 0.300418]\n",
      "8119 [D loss: 0.890899, acc.: 15.00%] [G loss: 0.415886]\n",
      "8139 [D loss: 0.713194, acc.: 50.00%] [G loss: 0.400310]\n",
      "8159 [D loss: 0.636421, acc.: 70.00%] [G loss: 0.353972]\n",
      "8179 [D loss: 0.743622, acc.: 30.00%] [G loss: 0.330376]\n",
      "8199 [D loss: 0.764351, acc.: 50.00%] [G loss: 0.307383]\n",
      "8219 [D loss: 0.805202, acc.: 25.00%] [G loss: 0.359297]\n",
      "8239 [D loss: 0.736539, acc.: 45.00%] [G loss: 0.368928]\n",
      "8259 [D loss: 0.613223, acc.: 55.00%] [G loss: 0.351829]\n",
      "8279 [D loss: 0.743647, acc.: 45.00%] [G loss: 0.330067]\n",
      "8299 [D loss: 0.732808, acc.: 40.00%] [G loss: 0.289608]\n",
      "8319 [D loss: 0.738507, acc.: 45.00%] [G loss: 0.268252]\n",
      "8339 [D loss: 0.863143, acc.: 30.00%] [G loss: 0.327685]\n",
      "8359 [D loss: 0.687110, acc.: 50.00%] [G loss: 0.305351]\n",
      "8379 [D loss: 0.722897, acc.: 45.00%] [G loss: 0.333503]\n",
      "8399 [D loss: 0.822150, acc.: 35.00%] [G loss: 0.298246]\n",
      "8419 [D loss: 0.705328, acc.: 55.00%] [G loss: 0.364838]\n",
      "8439 [D loss: 0.801150, acc.: 30.00%] [G loss: 0.359171]\n",
      "8459 [D loss: 0.668809, acc.: 60.00%] [G loss: 0.253347]\n",
      "8479 [D loss: 0.851184, acc.: 35.00%] [G loss: 0.312405]\n",
      "8499 [D loss: 0.809120, acc.: 15.00%] [G loss: 0.261965]\n",
      "8519 [D loss: 0.870370, acc.: 20.00%] [G loss: 0.401071]\n",
      "8539 [D loss: 0.689011, acc.: 35.00%] [G loss: 0.344908]\n",
      "8559 [D loss: 0.649433, acc.: 65.00%] [G loss: 0.339408]\n",
      "8579 [D loss: 0.864883, acc.: 25.00%] [G loss: 0.349998]\n",
      "8599 [D loss: 0.760430, acc.: 30.00%] [G loss: 0.281547]\n",
      "8619 [D loss: 0.823397, acc.: 50.00%] [G loss: 0.182455]\n",
      "8639 [D loss: 0.683085, acc.: 45.00%] [G loss: 0.373590]\n",
      "8659 [D loss: 0.772411, acc.: 30.00%] [G loss: 0.399916]\n",
      "8679 [D loss: 0.782921, acc.: 30.00%] [G loss: 0.385603]\n",
      "8699 [D loss: 0.725325, acc.: 60.00%] [G loss: 0.316118]\n",
      "8719 [D loss: 0.743975, acc.: 45.00%] [G loss: 0.366247]\n",
      "8739 [D loss: 0.807392, acc.: 10.00%] [G loss: 0.304838]\n",
      "8759 [D loss: 0.575297, acc.: 60.00%] [G loss: 0.294120]\n",
      "8779 [D loss: 0.830352, acc.: 35.00%] [G loss: 0.309173]\n",
      "8799 [D loss: 0.870839, acc.: 25.00%] [G loss: 0.352678]\n",
      "8819 [D loss: 0.704394, acc.: 50.00%] [G loss: 0.397262]\n",
      "8839 [D loss: 0.760923, acc.: 30.00%] [G loss: 0.301612]\n",
      "8859 [D loss: 0.774816, acc.: 35.00%] [G loss: 0.329115]\n",
      "8879 [D loss: 0.811298, acc.: 15.00%] [G loss: 0.321372]\n",
      "8899 [D loss: 0.608909, acc.: 60.00%] [G loss: 0.366521]\n",
      "8919 [D loss: 0.740358, acc.: 45.00%] [G loss: 0.307016]\n",
      "8939 [D loss: 0.722677, acc.: 35.00%] [G loss: 0.399783]\n",
      "8959 [D loss: 0.701696, acc.: 40.00%] [G loss: 0.406802]\n",
      "8979 [D loss: 0.684400, acc.: 45.00%] [G loss: 0.407560]\n",
      "8999 [D loss: 0.745692, acc.: 55.00%] [G loss: 0.309685]\n",
      "9019 [D loss: 0.733696, acc.: 40.00%] [G loss: 0.310162]\n",
      "9039 [D loss: 0.845333, acc.: 25.00%] [G loss: 0.259025]\n",
      "9059 [D loss: 0.883983, acc.: 30.00%] [G loss: 0.285969]\n",
      "9079 [D loss: 1.145936, acc.: 30.00%] [G loss: 0.257564]\n",
      "9099 [D loss: 0.877737, acc.: 15.00%] [G loss: 0.324465]\n",
      "9119 [D loss: 0.798324, acc.: 40.00%] [G loss: 0.331845]\n",
      "9139 [D loss: 0.865773, acc.: 50.00%] [G loss: 0.316432]\n",
      "9159 [D loss: 0.791995, acc.: 30.00%] [G loss: 0.272694]\n",
      "9179 [D loss: 0.618324, acc.: 60.00%] [G loss: 0.337297]\n",
      "9199 [D loss: 0.688762, acc.: 35.00%] [G loss: 0.322755]\n",
      "9219 [D loss: 0.822636, acc.: 50.00%] [G loss: 0.369085]\n",
      "9239 [D loss: 0.681267, acc.: 60.00%] [G loss: 0.375801]\n",
      "9259 [D loss: 0.765251, acc.: 40.00%] [G loss: 0.336934]\n",
      "9279 [D loss: 0.782606, acc.: 45.00%] [G loss: 0.340230]\n",
      "9299 [D loss: 0.697610, acc.: 55.00%] [G loss: 0.293412]\n",
      "9319 [D loss: 0.810970, acc.: 40.00%] [G loss: 0.268111]\n",
      "9339 [D loss: 0.669616, acc.: 50.00%] [G loss: 0.291487]\n",
      "9359 [D loss: 0.741414, acc.: 35.00%] [G loss: 0.345659]\n",
      "9379 [D loss: 0.753247, acc.: 30.00%] [G loss: 0.328903]\n",
      "9399 [D loss: 0.819027, acc.: 15.00%] [G loss: 0.292347]\n",
      "9419 [D loss: 0.849780, acc.: 35.00%] [G loss: 0.282195]\n",
      "9439 [D loss: 0.717185, acc.: 45.00%] [G loss: 0.318393]\n",
      "9459 [D loss: 0.750514, acc.: 25.00%] [G loss: 0.263596]\n",
      "9479 [D loss: 0.703153, acc.: 30.00%] [G loss: 0.319221]\n",
      "9499 [D loss: 0.602995, acc.: 70.00%] [G loss: 0.350848]\n",
      "9519 [D loss: 0.830187, acc.: 40.00%] [G loss: 0.309813]\n",
      "9539 [D loss: 0.790869, acc.: 30.00%] [G loss: 0.309169]\n",
      "9559 [D loss: 0.790004, acc.: 30.00%] [G loss: 0.310351]\n",
      "9579 [D loss: 0.821265, acc.: 25.00%] [G loss: 0.302767]\n",
      "9599 [D loss: 0.780409, acc.: 30.00%] [G loss: 0.319673]\n",
      "9619 [D loss: 0.756688, acc.: 35.00%] [G loss: 0.325589]\n",
      "9639 [D loss: 0.754433, acc.: 40.00%] [G loss: 0.314103]\n",
      "9659 [D loss: 0.834986, acc.: 20.00%] [G loss: 0.340583]\n",
      "9679 [D loss: 0.856706, acc.: 5.00%] [G loss: 0.301268]\n",
      "9699 [D loss: 0.895558, acc.: 10.00%] [G loss: 0.286113]\n",
      "9719 [D loss: 0.646287, acc.: 60.00%] [G loss: 0.334252]\n",
      "9739 [D loss: 0.860998, acc.: 25.00%] [G loss: 0.377858]\n",
      "9759 [D loss: 0.646713, acc.: 60.00%] [G loss: 0.452740]\n",
      "9779 [D loss: 0.742604, acc.: 30.00%] [G loss: 0.333297]\n",
      "9799 [D loss: 0.726094, acc.: 40.00%] [G loss: 0.262137]\n",
      "9819 [D loss: 0.672620, acc.: 65.00%] [G loss: 0.441633]\n",
      "9839 [D loss: 0.728729, acc.: 35.00%] [G loss: 0.360700]\n",
      "9859 [D loss: 0.717082, acc.: 40.00%] [G loss: 0.371229]\n",
      "9879 [D loss: 0.625624, acc.: 60.00%] [G loss: 0.327542]\n",
      "9899 [D loss: 0.749031, acc.: 40.00%] [G loss: 0.266745]\n",
      "9919 [D loss: 0.689737, acc.: 45.00%] [G loss: 0.296715]\n",
      "9939 [D loss: 0.710296, acc.: 35.00%] [G loss: 0.300154]\n",
      "9959 [D loss: 0.762550, acc.: 30.00%] [G loss: 0.403634]\n",
      "9979 [D loss: 0.757793, acc.: 50.00%] [G loss: 0.341512]\n",
      "9999 [D loss: 0.740901, acc.: 45.00%] [G loss: 0.311365]\n",
      "10019 [D loss: 0.653304, acc.: 65.00%] [G loss: 0.362169]\n",
      "10039 [D loss: 0.733644, acc.: 30.00%] [G loss: 0.346849]\n",
      "10059 [D loss: 0.791450, acc.: 25.00%] [G loss: 0.309942]\n",
      "10079 [D loss: 0.900860, acc.: 20.00%] [G loss: 0.296733]\n",
      "10099 [D loss: 0.867351, acc.: 40.00%] [G loss: 0.149044]\n",
      "10119 [D loss: 0.681365, acc.: 50.00%] [G loss: 0.300063]\n",
      "10139 [D loss: 0.711624, acc.: 40.00%] [G loss: 0.345015]\n",
      "10159 [D loss: 0.727385, acc.: 50.00%] [G loss: 0.320352]\n",
      "10179 [D loss: 0.785375, acc.: 30.00%] [G loss: 0.347978]\n",
      "10199 [D loss: 0.813071, acc.: 25.00%] [G loss: 0.286233]\n",
      "10219 [D loss: 0.725875, acc.: 35.00%] [G loss: 0.333560]\n",
      "10239 [D loss: 0.777787, acc.: 40.00%] [G loss: 0.315592]\n",
      "10259 [D loss: 0.723026, acc.: 35.00%] [G loss: 0.288815]\n",
      "10279 [D loss: 0.679144, acc.: 50.00%] [G loss: 0.319626]\n",
      "10299 [D loss: 0.810100, acc.: 15.00%] [G loss: 0.325214]\n",
      "10319 [D loss: 0.823509, acc.: 30.00%] [G loss: 0.343297]\n",
      "10339 [D loss: 0.693634, acc.: 35.00%] [G loss: 0.256932]\n",
      "10359 [D loss: 0.724144, acc.: 40.00%] [G loss: 0.287143]\n",
      "10379 [D loss: 0.759437, acc.: 55.00%] [G loss: 0.315674]\n",
      "10399 [D loss: 0.734231, acc.: 40.00%] [G loss: 0.317788]\n",
      "10419 [D loss: 0.827702, acc.: 20.00%] [G loss: 0.335578]\n",
      "10439 [D loss: 0.764747, acc.: 40.00%] [G loss: 0.329640]\n",
      "10459 [D loss: 0.793857, acc.: 5.00%] [G loss: 0.310853]\n",
      "10479 [D loss: 0.719629, acc.: 35.00%] [G loss: 0.280769]\n",
      "10499 [D loss: 0.792936, acc.: 40.00%] [G loss: 0.293510]\n",
      "10519 [D loss: 1.049660, acc.: 50.00%] [G loss: 0.283473]\n",
      "10539 [D loss: 0.758916, acc.: 45.00%] [G loss: 0.308627]\n",
      "10559 [D loss: 0.768594, acc.: 30.00%] [G loss: 0.316239]\n",
      "10579 [D loss: 0.853323, acc.: 15.00%] [G loss: 0.264116]\n",
      "10599 [D loss: 0.666580, acc.: 40.00%] [G loss: 0.304825]\n",
      "10619 [D loss: 0.745376, acc.: 20.00%] [G loss: 0.336316]\n",
      "10639 [D loss: 0.773469, acc.: 50.00%] [G loss: 0.297424]\n",
      "10659 [D loss: 0.754658, acc.: 35.00%] [G loss: 0.328468]\n",
      "10679 [D loss: 0.802258, acc.: 40.00%] [G loss: 0.341292]\n",
      "10699 [D loss: 0.796479, acc.: 25.00%] [G loss: 0.316369]\n",
      "10719 [D loss: 0.699941, acc.: 50.00%] [G loss: 0.341823]\n",
      "10739 [D loss: 0.672233, acc.: 60.00%] [G loss: 0.354160]\n",
      "10759 [D loss: 0.765613, acc.: 15.00%] [G loss: 0.288428]\n",
      "10779 [D loss: 0.768636, acc.: 45.00%] [G loss: 0.295754]\n",
      "10799 [D loss: 0.740366, acc.: 30.00%] [G loss: 0.333985]\n",
      "10819 [D loss: 0.742052, acc.: 35.00%] [G loss: 0.366318]\n",
      "10839 [D loss: 0.758903, acc.: 25.00%] [G loss: 0.300200]\n",
      "10859 [D loss: 0.734488, acc.: 35.00%] [G loss: 0.321674]\n",
      "10879 [D loss: 0.782503, acc.: 35.00%] [G loss: 0.315765]\n",
      "10899 [D loss: 0.878709, acc.: 20.00%] [G loss: 0.340106]\n",
      "10919 [D loss: 0.765914, acc.: 25.00%] [G loss: 0.356978]\n",
      "10939 [D loss: 0.742406, acc.: 20.00%] [G loss: 0.294509]\n",
      "10959 [D loss: 0.740194, acc.: 45.00%] [G loss: 0.341751]\n",
      "10979 [D loss: 0.728282, acc.: 35.00%] [G loss: 0.401018]\n",
      "10999 [D loss: 0.747618, acc.: 30.00%] [G loss: 0.327911]\n",
      "11019 [D loss: 0.786797, acc.: 20.00%] [G loss: 0.303436]\n",
      "11039 [D loss: 0.800119, acc.: 40.00%] [G loss: 0.337821]\n",
      "11059 [D loss: 0.791386, acc.: 40.00%] [G loss: 0.255935]\n",
      "11079 [D loss: 0.779950, acc.: 20.00%] [G loss: 0.301580]\n",
      "11099 [D loss: 0.767498, acc.: 30.00%] [G loss: 0.273437]\n",
      "11119 [D loss: 0.712649, acc.: 45.00%] [G loss: 0.353736]\n",
      "11139 [D loss: 0.751900, acc.: 35.00%] [G loss: 0.348827]\n",
      "11159 [D loss: 0.735297, acc.: 45.00%] [G loss: 0.343715]\n",
      "11179 [D loss: 0.730585, acc.: 40.00%] [G loss: 0.287183]\n",
      "11199 [D loss: 0.650277, acc.: 55.00%] [G loss: 0.306124]\n",
      "11219 [D loss: 0.758792, acc.: 25.00%] [G loss: 0.336022]\n",
      "11239 [D loss: 0.745326, acc.: 40.00%] [G loss: 0.314230]\n",
      "11259 [D loss: 0.791573, acc.: 40.00%] [G loss: 0.325750]\n",
      "11279 [D loss: 0.833635, acc.: 25.00%] [G loss: 0.275643]\n",
      "11299 [D loss: 0.790954, acc.: 30.00%] [G loss: 0.294010]\n",
      "11319 [D loss: 0.852444, acc.: 20.00%] [G loss: 0.358991]\n",
      "11339 [D loss: 0.704558, acc.: 55.00%] [G loss: 0.309580]\n",
      "11359 [D loss: 0.740817, acc.: 25.00%] [G loss: 0.298331]\n",
      "11379 [D loss: 0.782955, acc.: 25.00%] [G loss: 0.277729]\n",
      "11399 [D loss: 0.746287, acc.: 20.00%] [G loss: 0.320294]\n",
      "11419 [D loss: 0.754780, acc.: 30.00%] [G loss: 0.314771]\n",
      "11439 [D loss: 0.805829, acc.: 10.00%] [G loss: 0.319132]\n",
      "11459 [D loss: 0.671230, acc.: 50.00%] [G loss: 0.366534]\n",
      "11479 [D loss: 0.815282, acc.: 25.00%] [G loss: 0.339398]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d90658239b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m#     gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;31m#     gan.save_images(\"test\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-3d90658239b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# If at save interval => save generated image samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3d90658239b4>\u001b[0m in \u001b[0;36msave_images\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mipt_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0msam_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m#         # Rescale images 0 - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64 #28\n",
    "        self.img_cols = 64 #28\n",
    "        self.channels = 3 #1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Cut and load the dataset to shape (60000,64,64,3)\n",
    "        data = np.load(\"mnist_test_seq.npy\")\n",
    "        train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "        self.Y_train = train_set[3:6].transpose(1,2,3,0)\n",
    "        self.X_train = train_set[0:3].transpose(1,2,3,0)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "#         self.generator.compile(loss=['mean_squared_error','mean_absolute_error'],loss_weights=[0.001, 0.999],optimizer=optimizer)\n",
    "\n",
    "        # The generator takes ipt_imgs as input and generates gen_imgs\n",
    "        ipt_imgs = Input(shape=(self.img_shape))\n",
    "        gen_imgs = self.generator(ipt_imgs)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(gen_imgs)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(ipt_imgs, validity)\n",
    "        self.combined.compile(loss=['mean_squared_error','binary_crossentropy'],loss_weights=[0.999,0.001],optimizer=optimizer)\n",
    "        \n",
    "        # Set checkpoints and save trained models\n",
    "        self.checkpoint_dir = 'training_checkpoints/retrained'\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"tuned\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer,\n",
    "                                         discriminator_optimizer=optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        ipt_imgs = Input(shape=(self.img_shape))\n",
    "        gen_imgs = model(ipt_imgs)\n",
    "\n",
    "        return Model(ipt_imgs, gen_imgs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        sam_imgs = Input(shape=self.img_shape)\n",
    "        validity = model(sam_imgs)\n",
    "\n",
    "        return Model(sam_imgs, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=32 , sample_interval=30):\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        Y_train = self.Y_train / 127.5 - 1.0\n",
    "        X_train = self.X_train / 127.5 - 1.0\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size) \n",
    "            sam_imgs = Y_train[idx] #For Y_train\n",
    "            ipt_imgs = X_train[idx]\n",
    "            \n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(ipt_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(sam_imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(ipt_imgs, valid)\n",
    "\n",
    "            # Plot the progress every 20 epochs\n",
    "            if (epoch + 1) % sample_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # Save the models every 60 epochs\n",
    "            if (epoch + 1) % (3*sample_interval) == 0:\n",
    "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "                \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.save_images(epoch)\n",
    "\n",
    "\n",
    "    def save_images(self, epoch):\n",
    "\n",
    "        # Select Y_train and X_train\n",
    "        Y_train = self.Y_train\n",
    "        X_train = self.X_train\n",
    "        \n",
    "        # Select a clip for ploting\n",
    "        idx = np.random.randint(0, Y_train.shape[0], 10)\n",
    "        ipt_imgs = X_train[idx][0]\n",
    "        gen_imgs = self.generator.predict(X_train[idx])[0]\n",
    "        sam_imgs = Y_train[idx][0]\n",
    "\n",
    "#         # Rescale images 0 - 1\n",
    "#         gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "#         sam_imgs = 0.5 * sam_imgs + 0.5\n",
    "        \n",
    "        # Plot images\n",
    "        fig = plt.figure()\n",
    "        row1 = plt.subplot(3,3,1)\n",
    "        plt.imshow(sam_imgs[:,:,0], cmap='gray')\n",
    "        row1.title.set_text(\"Target sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,2)\n",
    "        plt.imshow(sam_imgs[:,:,1], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,3)\n",
    "        plt.imshow(sam_imgs[:,:,2], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        row2 = plt.subplot(3,3,4)\n",
    "        plt.imshow(ipt_imgs[:,:,0], cmap='gray')\n",
    "        row2.title.set_text(\"Input sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,5)\n",
    "        plt.imshow(ipt_imgs[:,:,1], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,6)\n",
    "        plt.imshow(ipt_imgs[:,:,2], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "                \n",
    "        row2 = plt.subplot(3,3,7)\n",
    "        plt.imshow(gen_imgs[:,:,0], cmap='gray')\n",
    "        row2.title.set_text(\"Generated sequence\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,8)\n",
    "        plt.imshow(gen_imgs[:,:,1], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(3,3,9)\n",
    "        plt.imshow(gen_imgs[:,:,2], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        crt_epoch = 'tuned'+str(epoch)\n",
    "        fig.savefig(\"generated_images/retrained/%s.png\" % crt_epoch) #%d\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = MIGAN()\n",
    "#     gan.checkpoint.restore(tf.train.latest_checkpoint(gan.checkpoint_dir))\n",
    "#     gan.save_images(\"test\")\n",
    "    gan.train(epochs=50000, batch_size=10, sample_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10000, 64, 64)\n",
      "(6, 90000, 64, 64)\n",
      "(90000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"mnist_test_seq.npy\")\n",
    "print(data.shape)\n",
    "train_set = np.concatenate((data[0:6],data[6:12],data[12:18],data[1:7],data[7:13],data[13:19],data[2:8],data[8:14],data[14:20]),axis=1)\n",
    "print(train_set.shape)\n",
    "Y_train = train_set[3:6].transpose(1,2,3,0)\n",
    "X_train = train_set[0:3].transpose(1,2,3,0)\n",
    "print(Y_train.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
